def llm_tool_sync(panel_prompt: str, msg: str) -> str:
    # 1) First call: allow tools
    r = oai.chat.completions.create(
        model=AZURE_OPENAI_DEPLOYMENT,
        messages=[
            {"role": "system", "content": panel_prompt},
            {"role": "user",   "content": msg},
        ],
        tools=TOOLS_SCHEMA,
        tool_choice="auto",
        max_tokens=300,
        temperature=0.7,
    )

    choice = r.choices[0]
    assistant_msg = choice.message
    tool_calls = getattr(assistant_msg, "tool_calls", None)

    # If the assistant decided to use tools
    if tool_calls:
        # 1a) Execute each tool
        tool_outputs = []
        for tc in tool_calls:
            fn = tc.function
            name = fn.name
            try:
                args = json.loads(fn.arguments or "{}")
            except Exception:
                args = {}
            result = _call_tool(name, args)
            tool_outputs.append((tc.id, name, result, fn.arguments))

        # 1b) Build the follow-up message list in the **required order**:
        # system, user, assistant(with tool_calls), tool, tool, ...
        msgs = [
            {"role": "system", "content": panel_prompt},
            {"role": "user",   "content": msg},
            {
                "role": "assistant",
                "content": assistant_msg.content or "",
                "tool_calls": [
                    {
                        "id": tc.id,
                        "type": "function",
                        "function": {
                            "name": tc.function.name,
                            "arguments": tc.function.arguments or "{}",
                        },
                    }
                    for tc in tool_calls
                ],
            },
        ]

        for tool_call_id, name, result, _raw_args in tool_outputs:
            msgs.append({
                "role": "tool",
                "tool_call_id": tool_call_id,
                "name": name,
                "content": json.dumps(result, ensure_ascii=False),
            })

        # 2) Second call: resolve with tool results (no new tool calls needed here)
        r2 = oai.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=msgs,
            max_tokens=350,
            temperature=0.7,
            # Optional: explicitly forbid new tools now
            tool_choice="none",
        )
        return (r2.choices[0].message.content or "").strip()

    # No tools usedâ€”return the assistant content from the first call
    return (assistant_msg.content or "").strip()
