# podcast_offline_version1.py â€” audio-only A/B debate (one sentence/turn, slower pace, 2â€“5 min)
# Fix: no nested asyncio.run inside event loop; pre-generate all speech, then write atomically
# Azure OpenAI (LLM) + Azure Speech (AAD)

import os, sys, re, math, wave, json, tempfile, asyncio, datetime, struct, calendar
from pathlib import Path
from dotenv import load_dotenv; load_dotenv()

# ===================== Azure OpenAI =====================
from openai import AzureOpenAI
AZURE_OPENAI_KEY        = os.getenv("AZURE_OPENAI_KEY") or os.getenv("OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT   = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")
OPENAI_API_VERSION      = os.getenv("OPENAI_API_VERSION", "2024-05-01-preview")
if not all([AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT, OPENAI_API_VERSION]):
    raise RuntimeError("Missing Azure OpenAI env vars")

oai = AzureOpenAI(api_key=AZURE_OPENAI_KEY,
                  azure_endpoint=AZURE_OPENAI_ENDPOINT,
                  api_version=OPENAI_API_VERSION)

def llm_sync(system: str, user: str, max_tokens: int, temperature: float) -> str:
    r = oai.chat.completions.create(
        model=AZURE_OPENAI_DEPLOYMENT,
        messages=[{"role":"system","content":system},{"role":"user","content":user}],
        max_tokens=max_tokens,
        temperature=temperature
    )
    return (r.choices[0].message.content or "").strip()

async def llm(system: str, user: str, max_tokens: int=150, temperature: float=0.5) -> str:
    return await asyncio.to_thread(llm_sync, system, user, max_tokens, temperature)

# ===================== Azure Speech (AAD) =====================
import azure.cognitiveservices.speech as speechsdk
from azure.identity import ClientSecretCredential

TENANT_ID     = os.getenv("TENANT_ID")
CLIENT_ID     = os.getenv("CLIENT_ID")
CLIENT_SECRET = os.getenv("CLIENT_SECRET")
SPEECH_REGION = os.getenv("SPEECH_REGION", "eastus")
RESOURCE_ID   = os.getenv("RESOURCE_ID")  # optional
COG_SCOPE     = "https://cognitiveservices.azure.com/.default"
if not all([TENANT_ID, CLIENT_ID, CLIENT_SECRET, SPEECH_REGION]):
    raise RuntimeError("Missing AAD Speech env vars (TENANT_ID, CLIENT_ID, CLIENT_SECRET, SPEECH_REGION)")

cred = ClientSecretCredential(tenant_id=TENANT_ID, client_id=CLIENT_ID, client_secret=CLIENT_SECRET)
def cog_token_str() -> str:
    tok = cred.get_token(COG_SCOPE).token
    return f"aad#{RESOURCE_ID}#{tok}" if RESOURCE_ID else tok

VOICE_A = os.getenv("VOICE_A", "en-US-GuyNeural")
VOICE_B = os.getenv("VOICE_B", "en-US-AriaNeural")

# ===================== Text shaping =====================
def first_sentence(text: str, max_words: int = 22) -> str:
    sents = re.split(r'(?<=[.!?])\s+', text.strip())
    s = sents[0] if sents and sents[0] else text.strip()
    words = s.split()
    if len(words) > max_words:
        cut = max_words
        for i in range(min(len(words), max_words+6), max(max_words-6, 1), -1):
            if words[i-1].endswith(('.', ',', ';', ':')):
                cut = i; break
        s = " ".join(words[:cut]).rstrip(",;:") + "."
    if not re.search(r'[.!?]$', s): s += "."
    return s

def _ymd_to_month_year(y, m):
    try:
        return f"{calendar.month_name[int(m)]} {y}"
    except Exception:
        return f"{y}-{m}"

def _round_thousands(num_str):
    try:
        n = float(num_str)
        if n >= 1000:
            val = round(n/1000.0, 1)
            return f"{val} thousand"
        return str(int(n)) if n.is_integer() else str(n)
    except Exception:
        return num_str

def speak_friendly(text: str) -> str:
    text = re.sub(r'\bASA\b', '<sub alias="A S A">ASA</sub>', text)
    text = re.sub(r'\b(\d{4})-(\d{2})-(\d{2})\b', lambda m: _ymd_to_month_year(m.group(1), m.group(2)), text)
    text = re.sub(r'\b(\d{4,})\b', lambda m: _round_thousands(m.group(1)), text)
    return text

def strip_markup(text: str) -> str:
    t = re.sub(r'[`*_#>]+', ' ', text)
    t = re.sub(r'\s{2,}', ' ', t).strip()
    return t

# ===================== SSML & TTS =====================
def ssml_wrapper(inner: str, voice: str, style: str | None, rate: str, pitch: str) -> str:
    inner = inner.strip() + " <break time='300ms'/>"
    if style:
        return f"""<speak version="1.0" xml:lang="en-US"
                 xmlns="http://www.w3.org/2001/10/synthesis"
                 xmlns:mstts="http://www.w3.org/2001/mstts">
  <voice name="{voice}">
    <mstts:express-as style="{style}">
      <prosody rate="{rate}" pitch="{pitch}">
        {inner}
      </prosody>
    </mstts:express-as>
  </voice>
</speak>"""
    else:
        return f"""<speak version="1.0" xml:lang="en-US"
                 xmlns="http://www.w3.org/2001/10/synthesis">
  <voice name="{voice}">
    <prosody rate="{rate}" pitch="{pitch}">
      {inner}
    </prosody>
  </voice>
</speak>"""

def tts_ssml_to_wav_with_fallback(text: str, voice: str, style: str | None, rate: str, pitch: str) -> str:
    cfg = speechsdk.SpeechConfig(auth_token=cog_token_str(), region=SPEECH_REGION)
    cfg.set_speech_synthesis_output_format(speechsdk.SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm)

    fd, tmp_path = tempfile.mkstemp(prefix="agent_tts_", suffix=".wav"); os.close(fd)
    try_styles = [style, None] if style else [None]

    for sty in try_styles:
        ssml = ssml_wrapper(text, voice, sty, rate, pitch)
        audio_cfg = speechsdk.audio.AudioOutputConfig(filename=tmp_path)
        synth = speechsdk.SpeechSynthesizer(speech_config=cfg, audio_config=audio_cfg)
        res = synth.speak_ssml_async(ssml).get()
        if res.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
            return tmp_path
    audio_cfg = speechsdk.audio.AudioOutputConfig(filename=tmp_path)
    synth = speechsdk.SpeechSynthesizer(speech_config=cfg, audio_config=audio_cfg)
    res = synth.speak_text_async(text).get()
    if res.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
        return tmp_path

    try: os.remove(tmp_path)
    except Exception: pass
    raise RuntimeError("TTS canceled even after SSML/style fallbacks")

def get_wav_duration_seconds(path: str) -> float:
    with wave.open(path, "rb") as r:
        frames = r.getnframes(); rate = r.getframerate()
        return frames / float(rate) if rate else 0.0

def micro_fade(wav_path: str, fade_ms: int = 8):
    try:
        with wave.open(wav_path, "rb") as r:
            nchan, sw, fr = r.getnchannels(), r.getsampwidth(), r.getframerate()
            frames = r.readframes(r.getnframes())
        if nchan != 1 or sw != 2 or fr != 24000:
            return
        import array
        data = array.array('h'); data.frombytes(frames)
        total = len(data); fade_samp = max(1, int(fr * fade_ms/1000.0))
        for i in range(min(fade_samp, total)): data[i] = int(data[i] * (i / fade_samp))
        for i in range(1, min(fade_samp, total)+1):
            idx = total - i; data[idx] = int(data[idx] * (i / fade_samp))
        with wave.open(wav_path, "wb") as w:
            w.setnchannels(1); w.setsampwidth(2); w.setframerate(24000)
            w.writeframes(data.tobytes())
    except Exception:
        pass

# ===================== Prompts (one sentence; senior tone) =====================
SYSTEM_A = (
"You are Agent A, a senior data analyst debating live with Agent B. "
"Return exactly one sentence (about 18â€“22 words). No lists, headings, hashtags, or filenames. "
"Use both datasets: weekly aggregates (2022â€“2025) and monthly KPIs (ASA seconds, call duration minutes, claim time days, and any others). "
"In that single sentence, synthesize across all metrics with concrete magnitudes and periods, explain the operational meaning, and end with a sharp question to Agent B."
)

SYSTEM_B = (
"You are Agent B, a senior strategist debating live with Agent A. "
"Return exactly one sentence (about 18â€“22 words). No lists or headings. "
"Use both datasets to diagnose drivers for Aâ€™s point (seasonality, staffing, routing/SLA, backlog, demand mix), "
"tie weekly aggregates to monthly KPIs across all metrics, challenge or refine A, and end with one concrete next step."
)

# ===================== File selection =====================
def ask_file_choice() -> str:
    base = Path(".").resolve()
    jsons = [p.name for p in base.iterdir() if p.suffix.lower() == ".json"]
    print("ğŸ“‚ JSON files in folder:", jsons)
    print("ğŸ‘‰ Type one of: data.json, metric.json, metric_data.json, both (recommended), or an exact filename, then Enter:")
    choice = sys.stdin.readline().strip().lower()
    return choice or "both"

def load_context(choice: str) -> tuple[str, dict]:
    ctx = ""
    meta = {"files":[]}
    def add_file(fname: str):
        p = Path(fname)
        if p.exists():
            meta["files"].append(fname)
            return f"[{fname}]\n{p.read_text(encoding='utf-8', errors='ignore')}\n\n"
        return ""
    if choice == "both":
        ctx += add_file("data.json")
        ctx += add_file("metric_data.json") or add_file("metric.json")
    else:
        p = Path(choice)
        if p.exists() and p.suffix.lower()==".json":
            ctx += add_file(p.name)
        else:
            if "data" in choice:         ctx += add_file("data.json")
            if "metric_data" in choice:  ctx += add_file("metric_data.json")
            if "metric.json" in choice:  ctx += add_file("metric.json")
    if not ctx:
        if Path("data.json").exists():           ctx += add_file("data.json")
        if Path("metric_data.json").exists():    ctx += add_file("metric_data.json")
        elif Path("metric.json").exists():       ctx += add_file("metric.json")
    if not ctx:
        raise RuntimeError("No data found (need data.json and/or metric_data.json / metric.json).")
    return ctx, meta

# ===================== Turn budgeting =====================
def estimate_turns(target_seconds: int) -> int:
    sec_per_exchange = 18.0   # A sentence + B sentence + pauses
    t = max(120, min(300, target_seconds))
    turns = max(4, int(t // sec_per_exchange))
    return min(turns, 16)

# ===================== Output paths =====================
OUT_WAV   = "podcast_offline_version1.wav"
SCRIPT_TXT= "podcast_script.txt"
SHOW_NOTES= "podcast_shownotes.md"
TRANS_JSON= "podcast_transcript.jsonl"
TITLE     = "Ops Signals â€” Data Debate"

# ===================== Build sentence -> TTS helpers =====================
def prepare_sentence_for_tts(text: str) -> str:
    clean = first_sentence(strip_markup(text))
    clean = speak_friendly(clean)
    return clean

def make_a_audio_sync(text: str) -> str:
    sent = prepare_sentence_for_tts(text)
    p = tts_ssml_to_wav_with_fallback(sent, VOICE_A, style="professional", rate="-7%", pitch="+0%")
    micro_fade(p)
    return p

def make_b_audio_sync(text: str) -> str:
    sent = prepare_sentence_for_tts(text)
    p = tts_ssml_to_wav_with_fallback(sent, VOICE_B, style="calm", rate="-6%", pitch="-1%")
    micro_fade(p)
    return p

async def make_a_audio(text: str) -> str:
    return await asyncio.to_thread(make_a_audio_sync, text)

async def make_b_audio(text: str) -> str:
    return await asyncio.to_thread(make_b_audio_sync, text)

# ===================== Safe WAV writing (atomic) =====================
def safe_write_podcast_wav(segments, final_path: str) -> float:
    """
    segments: list of dicts: {"type":"file","path":...} or {"type":"silence","ms":...}
    Writes to temp then atomic replace. Returns total duration seconds.
    """
    out_dir = Path(final_path).resolve().parent
    out_dir.mkdir(parents=True, exist_ok=True)
    fd, tmp_path = tempfile.mkstemp(prefix="podcast_", suffix=".tmp.wav", dir=out_dir)
    os.close(fd)
    total = 0.0
    master = None
    try:
        master = wave.open(tmp_path, "wb")
        master.setnchannels(1); master.setsampwidth(2); master.setframerate(24000)

        def append_silence(ms: int):
            rate = 24000
            samples = int(rate * (ms/1000.0))
            master.writeframes(b"\x00\x00" * samples)
            return ms/1000.0

        def append_file(path: str):
            with wave.open(path, "rb") as r:
                frames = r.readframes(r.getnframes())
                master.writeframes(frames)
            return get_wav_duration_seconds(path)

        for seg in segments:
            if seg["type"] == "file":
                total += append_file(seg["path"])
            elif seg["type"] == "silence":
                total += append_silence(seg["ms"])

        master.close(); master = None
        try:
            os.replace(tmp_path, final_path)
        except PermissionError:
            stamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            alt = str(Path(final_path).with_name(Path(final_path).stem + f"_{stamp}.wav"))
            os.replace(tmp_path, alt)
            print(f"âš ï¸ Could not overwrite {final_path} (in use?). Saved as {alt}")
    except Exception:
        try:
            if master is not None: master.close()
        except Exception:
            pass
        try:
            if os.path.exists(tmp_path): os.remove(tmp_path)
        except Exception:
            pass
        raise
    finally:
        # cleanup temp TTS files we created
        for seg in segments:
            if seg["type"] == "file":
                try: os.remove(seg["path"])
                except Exception: pass
    return total

# ===================== Main =====================
async def run_podcast():
    # 1) Ask files and duration
    choice = ask_file_choice()
    context, meta = load_context(choice)

    print("ğŸ›  Enter desired duration in minutes (2â€“5). Press Enter for default 3:")
    try:
        mins = sys.stdin.readline().strip()
        target_min = float(mins) if mins else 3.0
    except Exception:
        target_min = 3.0
    target_min = max(2.0, min(5.0, target_min))
    TARGET_SECONDS = int(target_min * 60)
    TURNS = estimate_turns(TARGET_SECONDS)

    transcript = []
    t0 = datetime.datetime.now().isoformat()
    Path(TRANS_JSON).write_text("", encoding="utf-8")

    def log(role: str, text: str, tsec: float):
        entry = {"ts": datetime.datetime.now().isoformat(), "t": round(tsec,2), "role": role, "text": text}
        transcript.append(f"{role}: {text}")
        with open(TRANS_JSON, "a", encoding="utf-8") as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")

    # 2) Build dialogue (LLM) and TTS **before** writing audio (no nested asyncio.run)
    # Intro chime & narration
    def synth_tone(duration_s=0.28, freq=520, vol=0.16, rate=24000) -> str:
        n = int(duration_s * rate)
        frames = bytearray()
        for i in range(n):
            samp = int(vol * 32767.0 * math.sin(2 * math.pi * freq * (i / rate)))
            frames += struct.pack('<h', samp)
        fd, p = tempfile.mkstemp(prefix="tone_", suffix=".wav"); os.close(fd)
        with wave.open(p, "wb") as w:
            w.setnchannels(1); w.setsampwidth(2); w.setframerate(rate)
            w.writeframes(frames)
        return p

    segments = []  # [{"type":"file","path":...} or {"type":"silence","ms":...}]
    elapsed = 0.0   # estimated while logging

    # chime + intro
    chime = synth_tone(0.28, 520, 0.16)
    segments.append({"type":"file","path": chime})
    segments.append({"type":"silence","ms": 140})

    intro = "Welcome to Ops Signals, where a senior analyst and strategist turn your metrics into decisions."
    wi = await make_a_audio(intro)
    segments.append({"type":"file","path": wi})
    segments.append({"type":"silence","ms": 200})
    log("Narration", first_sentence(intro), elapsed); elapsed += get_wav_duration_seconds(wi) + 0.34

    # Debate generation
    seed = f"Use these datasets as the only factual source. Keep conversation style; exactly one sentence per turn.\n{context[:12000]}"
    last = seed
    for _ in range(TURNS):
        # A
        a_out = await llm(SYSTEM_A, last, max_tokens=110, temperature=0.45)
        wa = await make_a_audio(a_out)
        segments.append({"type":"file","path": wa})
        segments.append({"type":"silence","ms": 160})
        log("Agent A", first_sentence(a_out), elapsed); elapsed += get_wav_duration_seconds(wa) + 0.16

        # B
        b_in = f"Agent A just said: {first_sentence(a_out)}\nReference (do not read filenames):\n{context[:9000]}"
        b_out = await llm(SYSTEM_B, b_in, max_tokens=110, temperature=0.45)
        wb = await make_b_audio(b_out)
        segments.append({"type":"file","path": wb})
        segments.append({"type":"silence","ms": 180})
        log("Agent B", first_sentence(b_out), elapsed); elapsed += get_wav_duration_seconds(wb) + 0.18

        last = b_out

    # Closing
    close = "Thatâ€™s the brief; if you add more files, weâ€™ll widen the lens and deepen the actions."
    wc = await make_b_audio(close)
    segments.append({"type":"file","path": wc})
    segments.append({"type":"silence","ms": 160})
    log("Narration", first_sentence(close), elapsed); elapsed += get_wav_duration_seconds(wc) + 0.16

    out = synth_tone(0.25, 500, 0.16)
    segments.append({"type":"file","path": out})

    # 3) SAFELY write podcast WAV (atomic)
    total_sec = safe_write_podcast_wav(segments, OUT_WAV)

    # 4) Script + Show notes
    with open(SCRIPT_TXT, "w", encoding="utf-8") as f:
        f.write("\n".join(transcript))

    with open(SHOW_NOTES, "w", encoding="utf-8") as f:
        f.write(f"# {TITLE}\n")
        f.write(f"- **Recorded**: {t0}\n")
        f.write(f"- **Duration**: ~{int(total_sec)}s\n")
        f.write(f"- **Files used**: {', '.join(meta['files'])}\n")
        f.write("\n## Structure\n")
        f.write("- Intro sting\n- One-sentence A/B debate (strict round-robin)\n- Closing sentence\n- Outro sting\n")
        f.write("\n## Notes\n")
        f.write("- Senior tone; all metrics considered; concrete magnitudes and periods; slower, clear delivery.\n")

    print(f"âœ… Saved: {OUT_WAV}, {SCRIPT_TXT}, {SHOW_NOTES}, {TRANS_JSON}")

if __name__ == "__main__":
    try:
        asyncio.run(run_podcast())
    except Exception as e:
        print(f"âŒ Error: {e}")
