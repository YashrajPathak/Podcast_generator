# podcast_offline_version1.py â€” Direct Podcast (~2 min, no interruptions)
# Azure OpenAI (LLM) + Azure Speech (AAD) + LiveKit RTC local audio track
# pip install -U azure-cognitiveservices-speech azure-identity openai python-dotenv livekit

import os, sys, asyncio, wave, tempfile, time
from pathlib import Path
from dotenv import load_dotenv; load_dotenv()

# ---------- Azure OpenAI ----------
from openai import AzureOpenAI
AZURE_OPENAI_KEY        = os.getenv("AZURE_OPENAI_KEY")
AZURE_OPENAI_ENDPOINT   = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")
OPENAI_API_VERSION      = os.getenv("OPENAI_API_VERSION", "2024-05-01-preview")

oai = AzureOpenAI(
    api_key=AZURE_OPENAI_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_version=OPENAI_API_VERSION,
)

async def llm(panel_prompt: str, msg: str) -> str:
    r = await asyncio.to_thread(lambda:
        oai.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[{"role": "system", "content": panel_prompt},
                      {"role": "user", "content": msg}],
            max_tokens=350,
            temperature=0.8
        )
    )
    return (r.choices[0].message.content or "").strip()

# ---------- Azure Speech (AAD) ----------
import azure.cognitiveservices.speech as speechsdk
from azure.identity import ClientSecretCredential

TENANT_ID     = os.getenv("TENANT_ID")
CLIENT_ID     = os.getenv("CLIENT_ID")
CLIENT_SECRET = os.getenv("CLIENT_SECRET")
SPEECH_REGION = os.getenv("SPEECH_REGION", "eastus")
RESOURCE_ID   = os.getenv("RESOURCE_ID")
COG_SCOPE     = "https://cognitiveservices.azure.com/.default"

cred = ClientSecretCredential(tenant_id=TENANT_ID, client_id=CLIENT_ID, client_secret=CLIENT_SECRET)
def cog_token_str() -> str:
    raw = cred.get_token(COG_SCOPE).token
    return f"aad#{RESOURCE_ID}#{raw}" if RESOURCE_ID else raw

def tts_to_pcm(text: str, voice: str) -> bytes:
    cfg = speechsdk.SpeechConfig(auth_token=cog_token_str(), region=SPEECH_REGION)
    cfg.speech_synthesis_voice_name = voice
    cfg.set_speech_synthesis_output_format(
        speechsdk.SpeechSynthesisOutputFormat.Raw24Khz16BitMonoPcm
    )
    synth = speechsdk.SpeechSynthesizer(speech_config=cfg, audio_config=None)
    res = synth.speak_text_async(text).get()
    if res.reason != speechsdk.ResultReason.SynthesizingAudioCompleted:
        raise RuntimeError(f"TTS failed: {res.reason}")
    return res.audio_data

def pcm_to_wav(pcm: bytes, wav_path: str):
    with wave.open(wav_path, "wb") as wf:
        wf.setnchannels(1); wf.setsampwidth(2); wf.setframerate(24000)
        wf.writeframes(pcm)

def get_pcm_duration(pcm: bytes) -> float:
    frames = len(pcm) // 2
    return frames / 24000.0

# ---------- LiveKit RTC (local) ----------
from livekit import rtc
SESSION_ID = "podcast-session"

async def publish_pcm_livekit(room: rtc.Room, agent: str, pcm: bytes):
    if not pcm: return
    source = rtc.AudioSource(sample_rate=24000, num_channels=1)
    track  = rtc.LocalAudioTrack.create_audio_track(f"{SESSION_ID}-{agent}", source)
    await room.local_participant.publish_track(track)
    frame_size = 24000 * 20 // 1000  # 20ms
    step = frame_size * 2
    for i in range(0, len(pcm), step):
        chunk = pcm[i:i+step]
        if len(chunk) < step: break
        frame = rtc.AudioFrame(chunk, 24000, 1, frame_size)
        await source.capture_frame(frame)
        await asyncio.sleep(0.02)
    print(f"ðŸ”Š [LiveKit:{agent}] published {len(pcm)//2} samples")

# ---------- Prompts ----------
SYSTEM_A = """You are Agent A, a data analyst.
You analyze operational data and metrics from two files:

1. data.json â€” weekly aggregates across 2022â€“2025 (sums, averages, min/max, YTD, MoM and VoV shifts).
2. metric.json â€” monthly KPIs including:
   - ASA (Average Speed of Answer) in seconds
   - Average Call Duration in minutes
   - Average Claim Processing Time in days

Your task:
- Provide a structured and detailed analysis of BOTH datasets together.
- Highlight anomalies: ASA spikes, sharp falls, large variances, YTD and MoM movements.
- Connect weekly aggregates (data.json) with granular KPIs (metric.json).
- Use comparisons (2024 vs 2025 YTD, etc.).
- Narrate clearly for audio, in natural concise sentences.
- Always end your turn with a probing analytical question for Agent B."""

SYSTEM_B = """You are Agent B, a strategist.
You interpret the insights Agent A describes, using both data.json and metric.json.

Your task:
- Explain plausible drivers for anomalies: seasonality, staffing, backlog, SLA shifts, customer demand.
- Connect macro weekly data to micro KPIs.
- Discuss risks, implications, and possible operational responses.
- Suggest practical actions (e.g., increase staffing, process redesign).
- Narrate clearly for audio, concise and structured.
- Always end by proposing a concrete next step or challenge back to Agent A."""

# ---------- File Loader ----------
def load_context() -> str:
    ctx = ""
    if Path("data.json").exists():
        ctx += "[data.json]\n" + Path("data.json").read_text(encoding="utf-8") + "\n\n"
    if Path("metric.json").exists():
        ctx += "[metric.json]\n" + Path("metric.json").read_text(encoding="utf-8")
    return ctx or "No data files found."

# ---------- Main ----------
TARGET_SECONDS = 120.0
VOICE_A = "en-US-GuyNeural"
VOICE_B = "en-US-AriaNeural"

async def run_podcast():
    transcript = []
    total_secs = 0.0
    pcm_all = b""

    room = rtc.Room()  # local room, no server
    print("ðŸ”Œ LiveKit local RTC session started.")

    # Greeting
    greet = "Hi Yashraj. Let's analyze your datasets and create a podcast summary."
    transcript.append(f"Agent A: {greet}")
    pcm = tts_to_pcm(greet, VOICE_A)
    pcm_all += pcm; total_secs += get_pcm_duration(pcm)
    await publish_pcm_livekit(room, "AgentA", pcm)

    # Load context
    context = load_context()
    print("ðŸ“š Context loaded from data.json + metric.json.\n")
    last = f"Here are the dataset contents:\n{context[:6000]}"

    # Exchanges until 2 min cap
    for _ in range(12):
        if total_secs >= TARGET_SECONDS: break

        # Agent A
        a_out = await llm(SYSTEM_A, last)
        transcript.append(f"Agent A: {a_out}")
        pcm = tts_to_pcm(a_out, VOICE_A)
        pcm_all += pcm; total_secs += get_pcm_duration(pcm)
        await publish_pcm_livekit(room, "AgentA", pcm)
        if total_secs >= TARGET_SECONDS: break

        # Agent B
        b_in = f"Agent A said: {a_out}\nReference datasets:\n{context[:5000]}"
        b_out = await llm(SYSTEM_B, b_in)
        transcript.append(f"Agent B: {b_out}")
        pcm = tts_to_pcm(b_out, VOICE_B)
        pcm_all += pcm; total_secs += get_pcm_duration(pcm)
        await publish_pcm_livekit(room, "AgentB", pcm)

        last = b_out

    # Closing
    if total_secs < TARGET_SECONDS - 2:
        closing = "That wraps our quick summary. Thanks for listening."
        transcript.append(f"Agent B: {closing}")
        pcm = tts_to_pcm(closing, VOICE_B)
        pcm_all += pcm; total_secs += get_pcm_duration(pcm)
        await publish_pcm_livekit(room, "AgentB", pcm)

    # Save final podcast
    pcm_to_wav(pcm_all, "podcast_offline_version1.wav")
    with open("podcast.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(transcript))
    print(f"âœ… Podcast saved as podcast_offline_version1.wav (~{int(total_secs)}s) and podcast.txt")

if __name__ == "__main__":
    asyncio.run(run_podcast())
