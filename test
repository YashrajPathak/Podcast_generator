# two_agents_podcast_v1.py â€” Direct Podcast (no interruptions, ~2 min total)
# Azure OpenAI (LLM) + Azure Speech (AAD) + Local podcast save
# pip install -U azure-cognitiveservices-speech azure-identity openai python-dotenv

import os, sys, asyncio, wave, tempfile
from pathlib import Path
from dotenv import load_dotenv; load_dotenv()

# ---------- Azure OpenAI ----------
from openai import AzureOpenAI
AZURE_OPENAI_KEY        = os.getenv("AZURE_OPENAI_KEY") or os.getenv("OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT   = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")
OPENAI_API_VERSION      = os.getenv("OPENAI_API_VERSION", "2024-05-01-preview")

if not all([AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT, OPENAI_API_VERSION]):
    raise RuntimeError("Missing Azure OpenAI env vars")

oai = AzureOpenAI(
    api_key=AZURE_OPENAI_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_version=OPENAI_API_VERSION,
)

async def llm(panel_prompt: str, msg: str) -> str:
    r = await asyncio.to_thread(lambda:
        oai.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[{"role": "system", "content": panel_prompt},
                      {"role": "user", "content": msg}],
            max_tokens=300,
            temperature=0.8
        )
    )
    return (r.choices[0].message.content or "").strip()

# ---------- Azure Speech (AAD) ----------
import azure.cognitiveservices.speech as speechsdk
from azure.identity import ClientSecretCredential

TENANT_ID     = os.getenv("TENANT_ID")
CLIENT_ID     = os.getenv("CLIENT_ID")
CLIENT_SECRET = os.getenv("CLIENT_SECRET")
SPEECH_REGION = os.getenv("SPEECH_REGION", "eastus")
RESOURCE_ID   = os.getenv("RESOURCE_ID")
COG_SCOPE     = "https://cognitiveservices.azure.com/.default"

if not all([TENANT_ID, CLIENT_ID, CLIENT_SECRET, SPEECH_REGION]):
    raise RuntimeError("Missing AAD Speech env vars (TENANT_ID, CLIENT_ID, CLIENT_SECRET, SPEECH_REGION)")

cred = ClientSecretCredential(tenant_id=TENANT_ID, client_id=CLIENT_ID, client_secret=CLIENT_SECRET)
def cog_token_str() -> str:
    raw = cred.get_token(COG_SCOPE).token
    return f"aad#{RESOURCE_ID}#{raw}" if RESOURCE_ID else raw

def tts_to_temp_wav(text: str, voice: str) -> str:
    """Synthesize text to a RIFF WAV temp file (24kHz/16-bit mono) and return the path."""
    cfg = speechsdk.SpeechConfig(auth_token=cog_token_str(), region=SPEECH_REGION)
    cfg.speech_synthesis_voice_name = voice
    cfg.set_speech_synthesis_output_format(
        speechsdk.SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm
    )
    fd, tmp_path = tempfile.mkstemp(prefix="agent_tts_", suffix=".wav"); os.close(fd)
    audio_cfg = speechsdk.audio.AudioOutputConfig(filename=tmp_path)
    synth = speechsdk.SpeechSynthesizer(speech_config=cfg, audio_config=audio_cfg)
    res = synth.speak_text_async(text).get()
    if res.reason != speechsdk.ResultReason.SynthesizingAudioCompleted:
        try: os.remove(tmp_path)
        except Exception: pass
        raise RuntimeError(f"TTS failed: {res.reason}")
    return tmp_path

def get_wav_duration_seconds(path: str) -> float:
    with wave.open(path, "rb") as r:
        frames = r.getnframes()
        rate   = r.getframerate()
        return frames / float(rate) if rate else 0.0

def append_wav_into_master(src_path: str, master_wf: wave.Wave_write):
    with wave.open(src_path, "rb") as r:
        frames = r.readframes(r.getnframes())
        master_wf.writeframes(frames)

# ---------- Prompts (tailored for your data.json + metric.json) ----------
SYSTEM_A = """You are Agent A, a data analyst.
You analyze operational data and metrics from two files:
1. data.json â€” weekly aggregates across 2022â€“2025 (sums, averages, min/max, YTD, MoM and VoV shifts).
2. metric.json â€” monthly KPIs (ASA in seconds, Average Call Duration in minutes, Claim Processing Time in days).

Your task:
- Summarize key trends and anomalies from BOTH datasets with specific references (e.g., ASA spikes/falls, claim-time jumps, call-duration shifts, YTD changes).
- Compare 2024 vs 2025 YTD and relate macro movements (data.json) to granular KPIs (metric.json).
- Prioritize clear, structured narration that would sound good in audio.
- Speak concisely (2â€“3 sentences) and end with a probing analytical question for Agent B."""

SYSTEM_B = """You are Agent B, a strategist.
You interpret patterns described by Agent A using both data.json and metric.json.

Your task:
- Provide plausible drivers for anomalies (seasonality, staffing, policy, demand mix, backlog effects).
- Discuss risks, implications, and operational levers (headcount, routing, SLAs, self-service, backlog triage).
- Tie the weekly aggregates to the KPI movements and propose next steps.
- Speak concisely (2â€“3 sentences) and end by proposing concrete actions or a challenge for Agent A."""

# ---------- File Loader ----------
def load_context() -> str:
    base = Path(".").resolve()
    jsons = [f for f in base.iterdir() if f.suffix.lower() == ".json"]
    print("ðŸ“‚ JSON files found:", [f.name for f in jsons])
    print("ðŸ‘‰ Type one of: data.json, metric.json, both (to combine), or a specific json filename:")
    choice = sys.stdin.readline().strip().lower()

    # Helper to read file if present
    def readf(name: str) -> str:
        p = Path(name)
        return p.read_text(encoding="utf-8") if p.exists() else ""

    if "both" in choice or ("data" in choice and "metric" in choice):
        ctx = ""
        if Path("data.json").exists():
            ctx += "[data.json]\n" + readf("data.json") + "\n\n"
        if Path("metric.json").exists():
            ctx += "[metric.json]\n" + readf("metric.json")
        if ctx:
            return ctx
    # else try exact or partial
    for f in jsons:
        if f.name.lower() in choice or f.stem.lower() in choice:
            return f"[{f.name}]\n" + f.read_text(encoding="utf-8")
    # default fallback: prefer data+metric if present
    combined = ""
    if Path("data.json").exists():   combined += "[data.json]\n"   + Path("data.json").read_text(encoding="utf-8") + "\n\n"
    if Path("metric.json").exists(): combined += "[metric.json]\n" + Path("metric.json").read_text(encoding="utf-8")
    return combined or "No data."

# ---------- Main (2-minute cap) ----------
TARGET_SECONDS = 120.0
VOICE_A = "en-US-GuyNeural"
VOICE_B = "en-US-AriaNeural"

async def run_podcast():
    transcript = []
    total_secs = 0.0

    with wave.open("podcast.wav", "wb") as master_wf:
        master_wf.setnchannels(1); master_wf.setsampwidth(2); master_wf.setframerate(24000)

        # Greeting
        greet = "Hi Yashraj. Tell me the dataset to analyze: type data.json, metric.json, or both."
        print(f"\nðŸŸ¦ Agent A (greeting): {greet}\n")
        transcript.append(f"Agent A: {greet}")
        wav = tts_to_temp_wav(greet, VOICE_A)
        append_wav_into_master(wav, master_wf)
        total_secs += get_wav_duration_seconds(wav)
        os.remove(wav)

        # Load context from your choice
        context = load_context()
        print("ðŸ“š Context loaded. Building a concise, two-minute podcast...\n")

        # Debate until ~2 minutes total (including greeting)
        last = f"Here are the files' contents (truncated for context):\n{context[:4000]}"

        # Small upper bound on exchanges; duration will stop earlier once we hit ~120s
        for _ in range(12):
            # Agent A
            if total_secs >= TARGET_SECONDS: break
            a_out = await llm(SYSTEM_A, last)
            print(f"\nðŸŸ¦ Agent A: {a_out}\n"); transcript.append(f"Agent A: {a_out}")
            wav = tts_to_temp_wav(a_out, VOICE_A)
            append_wav_into_master(wav, master_wf)
            total_secs += get_wav_duration_seconds(wav)
            os.remove(wav)
            if total_secs >= TARGET_SECONDS: break

            # Agent B
            b_in = f"Agent A said: {a_out}\nReference dataset:\n{context[:3000]}"
            b_out = await llm(SYSTEM_B, b_in)
            print(f"\nðŸŸ© Agent B: {b_out}\n"); transcript.append(f"Agent B: {b_out}")
            wav = tts_to_temp_wav(b_out, VOICE_B)
            append_wav_into_master(wav, master_wf)
            total_secs += get_wav_duration_seconds(wav)
            os.remove(wav)

            last = b_out
            if total_secs >= TARGET_SECONDS: break

        # Optional: short closing if we still have a couple seconds left
        if total_secs < TARGET_SECONDS - 2.0:
            closing = "That wraps our quick summary. Thanks for listening."
            transcript.append(f"Agent B: {closing}")
            wav = tts_to_temp_wav(closing, VOICE_B)
            append_wav_into_master(wav, master_wf)
            total_secs += get_wav_duration_seconds(wav)
            os.remove(wav)

    with open("podcast.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(transcript))
    print(f"âœ… Podcast saved as podcast.wav (~{int(total_secs)}s) and podcast.txt")

if __name__ == "__main__":
    asyncio.run(run_podcast())
