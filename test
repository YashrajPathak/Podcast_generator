# two_agents_podcast_offline_v1.py — Direct Podcast (~2 min, no interruptions, audio only)
# Azure OpenAI (LLM) + Azure Speech (AAD) + Local LiveKit rtc graph (no server needed)
# pip install -U azure-cognitiveservices-speech azure-identity openai python-dotenv livekit

import os, sys, asyncio, wave, tempfile, time, platform
from pathlib import Path
from dotenv import load_dotenv; load_dotenv()

# ---------- Azure OpenAI ----------
from openai import AzureOpenAI
AZURE_OPENAI_KEY        = os.getenv("AZURE_OPENAI_KEY") or os.getenv("OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT   = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")
OPENAI_API_VERSION      = os.getenv("OPENAI_API_VERSION", "2024-05-01-preview")
if not all([AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT, OPENAI_API_VERSION]):
    raise RuntimeError("Missing Azure OpenAI env vars")

oai = AzureOpenAI(
    api_key=AZURE_OPENAI_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_version=OPENAI_API_VERSION,
)

async def llm(panel_prompt: str, msg: str, max_tokens: int = 320) -> str:
    r = await asyncio.to_thread(lambda:
        oai.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[{"role": "system", "content": panel_prompt},
                      {"role": "user",   "content": msg}],
            max_tokens=max_tokens,
            temperature=0.7
        )
    )
    return (r.choices[0].message.content or "").strip()

# ---------- Azure Speech (AAD) ----------
import azure.cognitiveservices.speech as speechsdk
from azure.identity import ClientSecretCredential

TENANT_ID     = os.getenv("TENANT_ID")
CLIENT_ID     = os.getenv("CLIENT_ID")
CLIENT_SECRET = os.getenv("CLIENT_SECRET")
SPEECH_REGION = os.getenv("SPEECH_REGION", "eastus")
RESOURCE_ID   = os.getenv("RESOURCE_ID")  # optional
COG_SCOPE     = "https://cognitiveservices.azure.com/.default"
if not all([TENANT_ID, CLIENT_ID, CLIENT_SECRET, SPEECH_REGION]):
    raise RuntimeError("Missing AAD Speech env vars (TENANT_ID, CLIENT_ID, CLIENT_SECRET, SPEECH_REGION)")

cred = ClientSecretCredential(tenant_id=TENANT_ID, client_id=CLIENT_ID, client_secret=CLIENT_SECRET)
def cog_token_str() -> str:
    raw = cred.get_token(COG_SCOPE).token
    return f"aad#{RESOURCE_ID}#{raw}" if RESOURCE_ID else raw

def tts_to_temp_wav(text: str, voice: str = "en-US-GuyNeural") -> str:
    cfg = speechsdk.SpeechConfig(auth_token=cog_token_str(), region=SPEECH_REGION)
    cfg.speech_synthesis_voice_name = voice
    cfg.set_speech_synthesis_output_format(
        speechsdk.SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm
    )
    fd, tmp_path = tempfile.mkstemp(prefix="agent_tts_", suffix=".wav"); os.close(fd)
    audio_cfg = speechsdk.audio.AudioOutputConfig(filename=tmp_path)
    synth = speechsdk.SpeechSynthesizer(speech_config=cfg, audio_config=audio_cfg)
    res = synth.speak_text_async(text).get()
    if res.reason != speechsdk.ResultReason.SynthesizingAudioCompleted:
        try: os.remove(tmp_path)
        except Exception: pass
        raise RuntimeError(f"TTS failed: {res.reason}")
    return tmp_path

def wav_seconds(path: str) -> float:
    with wave.open(path, "rb") as r:
        frames = r.getnframes(); rate = r.getframerate()
        return frames / float(rate) if rate else 0.0

def append_wav_into_master(src_path: str, master_wf: wave.Wave_write):
    with wave.open(src_path, "rb") as r:
        frames = r.readframes(r.getnframes())
        master_wf.writeframes(frames)

# ---------- LiveKit local RTC (no server/keys) ----------
from livekit import rtc
SESSION_ID = "offline-podcast-v1"
SAMPLE_RATE = 24000
CHANNELS = 1
SAMPLE_WIDTH = 2   # 16-bit
FRAME_MS = 20
SAMPLES_PER_CH  = SAMPLE_RATE * FRAME_MS // 1000
BYTES_PER_FRAME = SAMPLES_PER_CH * CHANNELS * SAMPLE_WIDTH

def _track_name(agent: str) -> str:
    return f"{SESSION_ID}-{agent}-voice"

async def livekit_room_local() -> rtc.Room:
    # Not connecting anywhere; we just use the local rtc graph to process frames.
    return rtc.Room()  # no connect()

async def publish_wav_via_livekit(room: rtc.Room, agent_label: str, wav_path: str):
    """
    Create an AudioSource + LocalAudioTrack and feed frames from the WAV.
    This exercises the real LiveKit capture path locally (no server).
    """
    src = rtc.AudioSource(sample_rate=SAMPLE_RATE, num_channels=CHANNELS)
    track = rtc.LocalAudioTrack.create_audio_track(_track_name(agent_label), src)
    # In local-only mode, publish doesn't transmit—still safe to call:
    try:
        await room.local_participant.publish_track(track)
    except Exception:
        pass

    with wave.open(wav_path, "rb") as r:
        # Expecting 24k/16-bit/mono from our TTS
        raw = r.readframes(r.getnframes())
    # Feed in 20ms slices
    for i in range(0, len(raw), BYTES_PER_FRAME):
        seg = raw[i:i+BYTES_PER_FRAME]
        if len(seg) < BYTES_PER_FRAME:
            break
        frame = rtc.AudioFrame(seg, SAMPLE_RATE, CHANNELS, SAMPLES_PER_CH)
        await src.capture_frame(frame)
        await asyncio.sleep(FRAME_MS / 1000)

# ---------- Debate prompts (conversational, senior tone, NO markdown) ----------
VOICE_A = "en-US-GuyNeural"
VOICE_B = "en-US-AriaNeural"
TARGET_SECONDS = 120.0  # ~2 minutes

SYSTEM_A = """You are Agent A, a PRINCIPAL data analyst speaking in a live conversation.
You have two datasets: data.json (weekly aggregates 2022–2025 with sums, averages, min/max, YTD, MoM and WoW shifts) and metric.json (monthly KPIs: ASA in seconds, Average Call Duration in minutes, Claim Processing Time in days).
Speak like a senior analyst to another senior analyst. Use specific numbers you see. Explain what those numbers mean and why they matter. Focus on one or two observations per turn, keep it natural and reactive.
STRICT RULES: no markdown, no lists, no headings, no hashtags, no asterisks. 2–3 sentences max. End with a probing question that invites Agent B to respond."""

SYSTEM_B = """You are Agent B, Head of Strategy, speaking in a live conversation with Agent A.
You listen to Agent A’s point and respond directly. Interpret the patterns with plausible drivers: seasonality, staffing, backlog, mix, policy, SLAs. Tie weekly aggregates to the KPIs. Use concrete values where relevant, challenge if needed, propose next actions.
STRICT RULES: no markdown, no lists, no headings, no hashtags, no asterisks. 2–3 sentences max. End with a concrete follow-up prompt for Agent A."""

# ---------- Context loader (auto-pick both if present) ----------
def load_context() -> str:
    ctx = ""
    if Path("data.json").exists():
        ctx += "[data.json]\n" + Path("data.json").read_text(encoding="utf-8") + "\n\n"
    if Path("metric.json").exists():
        ctx += "[metric.json]\n" + Path("metric.json").read_text(encoding="utf-8")
    if not ctx:
        # fallback: any json
        jsons = [p for p in Path(".").iterdir() if p.suffix.lower()==".json"]
        for p in jsons:
            ctx += f"[{p.name}]\n{p.read_text(encoding='utf-8')}\n\n"
    return ctx or "No data."

# ---------- Main (no on-screen dialogue; just final audio + silent transcript) ----------
async def run_podcast():
    # Minimal console noise:
    print("🎧 Generating podcast… (audio only)")
    room = await livekit_room_local()

    transcript = []  # saved quietly for QA
    total_secs = 0.0
    master_name = "podcast_offline_v1.wav"

    with wave.open(master_name, "wb") as master_wf:
        master_wf.setnchannels(1); master_wf.setsampwidth(2); master_wf.setframerate(24000)

        # Greeting kept short to save time for debate
        greet = "Hi Yashraj. I will generate a concise two minute podcast summary from your files."
        wav = tts_to_temp_wav(greet, VOICE_A)
        append_wav_into_master(wav, master_wf)
        await publish_wav_via_livekit(room, "Agent A", wav)
        total_secs += wav_seconds(wav); os.remove(wav)
        transcript.append(f"Agent A: {greet}")

        context = load_context()
        last = f"Use these datasets together. Speak naturally with numbers; no lists:\n{context[:7500]}"

        # Short, reactive turns; stop near 2 minutes
        # Target ~8–10 turns total depending on TTS length
        for turn in range(12):
            if total_secs >= TARGET_SECONDS: break

            # Agent A
            a_out = await llm(SYSTEM_A,
                              last + "\nFocus on one or two anomalies first. Keep it tight. No markdown.",
                              max_tokens=220)
            wav = tts_to_temp_wav(a_out, VOICE_A)
            append_wav_into_master(wav, master_wf)
            await publish_wav_via_livekit(room, "Agent A", wav)
            total_secs += wav_seconds(wav); os.remove(wav)
            transcript.append(f"Agent A: {a_out}")
            if total_secs >= TARGET_SECONDS: break

            # Agent B responds to A (reactive)
            b_in = f"Agent A said: {a_out}\nNow respond as Agent B, concise, natural, challenge or extend, propose next action. No markdown."
            b_out = await llm(SYSTEM_B, b_in, max_tokens=220)
            wav = tts_to_temp_wav(b_out, VOICE_B)
            append_wav_into_master(wav, master_wf)
            await publish_wav_via_livekit(room, "Agent B", wav)
            total_secs += wav_seconds(wav); os.remove(wav)
            transcript.append(f"Agent B: {b_out}")

            last = b_out  # keep the dialogue reactive

        # Tight closing if room left
        if total_secs < TARGET_SECONDS - 1.8:
            closing = "That wraps the discussion. Thanks for listening."
            wav = tts_to_temp_wav(closing, VOICE_B)
            append_wav_into_master(wav, master_wf)
            await publish_wav_via_livekit(room, "Agent B", wav)
            total_secs += wav_seconds(wav); os.remove(wav)
            transcript.append(f"Agent B: {closing}")

    # Silent transcript for QA (don’t print)
    with open("podcast_offline_v1.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(transcript))

    print(f"✅ Done. Saved: {master_name}")

if __name__ == "__main__":
    try:
        asyncio.run(run_podcast())
    except KeyboardInterrupt:
        print("\n⏹️ Stopped.")
