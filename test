# podcast_offline_version1.py — 2-minute audio-only debate (A/B), emotion + senior analysis
# Azure OpenAI (LLM) + Azure Speech (AAD, SSML) — no console dialogue, final WAV only
# pip install -U azure-cognitiveservices-speech azure-identity openai python-dotenv

import os, sys, asyncio, wave, tempfile, re, json
from pathlib import Path
from dotenv import load_dotenv; load_dotenv()

# ========== Azure OpenAI ==========
from openai import AzureOpenAI
AZURE_OPENAI_KEY        = os.getenv("AZURE_OPENAI_KEY") or os.getenv("OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT   = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")
OPENAI_API_VERSION      = os.getenv("OPENAI_API_VERSION", "2024-05-01-preview")
if not all([AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT, OPENAI_API_VERSION]):
    raise RuntimeError("Missing Azure OpenAI env vars")

oai = AzureOpenAI(api_key=AZURE_OPENAI_KEY, azure_endpoint=AZURE_OPENAI_ENDPOINT, api_version=OPENAI_API_VERSION)

def llm_sync(panel_prompt: str, msg: str, max_tokens: int = 260, temperature: float = 0.6) -> str:
    r = oai.chat.completions.create(
        model=AZURE_OPENAI_DEPLOYMENT,
        messages=[{"role":"system","content":panel_prompt},{"role":"user","content":msg}],
        max_tokens=max_tokens, temperature=temperature)
    return (r.choices[0].message.content or "").strip()

async def llm(panel_prompt: str, msg: str, max_tokens: int = 260, temperature: float = 0.6) -> str:
    return await asyncio.to_thread(llm_sync, panel_prompt, msg, max_tokens, temperature)

# ========== Azure Speech (AAD, SSML) ==========
import azure.cognitiveservices.speech as speechsdk
from azure.identity import ClientSecretCredential

TENANT_ID     = os.getenv("TENANT_ID")
CLIENT_ID     = os.getenv("CLIENT_ID")
CLIENT_SECRET = os.getenv("CLIENT_SECRET")
SPEECH_REGION = os.getenv("SPEECH_REGION", "eastus")
RESOURCE_ID   = os.getenv("RESOURCE_ID")  # optional
COG_SCOPE     = "https://cognitiveservices.azure.com/.default"
if not all([TENANT_ID, CLIENT_ID, CLIENT_SECRET, SPEECH_REGION]):
    raise RuntimeError("Missing AAD Speech env vars (TENANT_ID, CLIENT_ID, CLIENT_SECRET, SPEECH_REGION)")

cred = ClientSecretCredential(tenant_id=TENANT_ID, client_id=CLIENT_ID, client_secret=CLIENT_SECRET)
def cog_token_str() -> str:
    raw = cred.get_token(COG_SCOPE).token
    return f"aad#{RESOURCE_ID}#{raw}" if RESOURCE_ID else raw

VOICE_A = os.getenv("VOICE_A", "en-US-GuyNeural")   # analyst
VOICE_B = os.getenv("VOICE_B", "en-US-AriaNeural")  # strategist

def to_ssml(text: str, voice: str, style: str, rate: str = "0%", pitch: str = "0%") -> str:
    # Split into sentences and add brief pauses for conversational flow
    sents = re.split(r'(?<=[.!?])\s+', text.strip())
    safe = " ".join(sents)  # fallback if sentence split is odd
    parts = []
    for i, s in enumerate(sents):
        s = s.strip()
        if not s: continue
        # light pause after each sentence for natural cadence
        parts.append(f"{s}<break time='220ms'/>")
    inner = " ".join(parts) if parts else safe
    # SSML with expressive style (supported styles vary by voice; these are commonly available)
    return f"""<speak version="1.0" xml:lang="en-US">
  <voice name="{voice}">
    <mstts:express-as style="{style}">
      <prosody rate="{rate}" pitch="{pitch}">
        {inner}
      </prosody>
    </mstts:express-as>
  </voice>
</speak>"""

def tts_ssml_to_wav(ssml: str) -> str:
    cfg = speechsdk.SpeechConfig(auth_token=cog_token_str(), region=SPEECH_REGION)
    # We output RIFF 24kHz / 16-bit mono to append seamlessly
    cfg.set_speech_synthesis_output_format(speechsdk.SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm)
    fd, tmp_path = tempfile.mkstemp(prefix="agent_tts_", suffix=".wav"); os.close(fd)
    audio_cfg = speechsdk.audio.AudioOutputConfig(filename=tmp_path)
    synth = speechsdk.SpeechSynthesizer(speech_config=cfg, audio_config=audio_cfg)
    res = synth.speak_ssml_async(ssml).get()
    if res.reason != speechsdk.ResultReason.SynthesizingAudioCompleted:
        try: os.remove(tmp_path)
        except Exception: pass
        raise RuntimeError(f"TTS failed: {res.reason}")
    return tmp_path

def get_wav_duration_seconds(path: str) -> float:
    with wave.open(path, "rb") as r:
        frames = r.getnframes(); rate = r.getframerate()
        return frames / float(rate) if rate else 0.0

def append_wav_into_master(src_path: str, master_wf: wave.Wave_write):
    with wave.open(src_path, "rb") as r:
        frames = r.readframes(r.getnframes())
        master_wf.writeframes(frames)

def strip_markup(text: str) -> str:
    # remove markdown bullets/headers/hashtags/inline code for clean speech
    t = re.sub(r'[`*_#>]+', ' ', text)        # markdown tokens
    t = re.sub(r'\s{2,}', ' ', t).strip()
    return t

# ========== Debate Prompts (expert, emotional, round-robin) ==========
# Key design: no lists, no headings; cover ALL metrics present; challenge/synthesize; concrete numbers.
SYSTEM_A = """
You are Agent A, a senior data analyst speaking live on a podcast with Agent B (a strategist).
Speak in natural, conversational English for audio. No lists, no headings, no hashtags, no file names.

Responsibilities:
- You’ve read two datasets: data.json (weekly aggregates 2022–2025, incl. YTD, MoM/WoW) and metric.json (monthly KPIs: ASA in seconds, Average Call Duration in minutes, Claim Processing Time in days, and ANY other metrics present).
- In each turn (2–3 short sentences), cite specific figures (e.g., ranges, YTD deltas, notable spikes/drops by month) and explain what they MEAN operationally.
- Always cover the breadth of metrics over time, not just one KPI; if multiple exist, weave them naturally (e.g., “as ASA rose, claim time fell…”).
- Sound confident and human: vary emphasis, contrast ideas, use rhetorical questions sparingly; avoid reading bullets or meta text.
- End each turn with a pointed question that nudges B to explain causes or tradeoffs.
- Stay concise: about 35–55 spoken words per turn.
"""

SYSTEM_B = """
You are Agent B, a senior strategist debating live with Agent A.
Speak in natural, conversational English for audio. No lists, no headings, no hashtags, no file names.

Responsibilities:
- Listen to A’s last turn and respond directly in 2–3 short sentences.
- Diagnose plausible drivers (seasonality, staffing, backlog dynamics, routing/SLA policy, demand mix) and connect weekly aggregates to monthly KPIs across ALL metrics present.
- Challenge or refine A’s claim with evidence (numbers/directions from the datasets) and propose a sharp next step or tradeoff.
- Keep it tight (35–55 words) and end with an action-oriented challenge back to A.
"""

# ========== Data Loader ==========
def load_context() -> str:
    ctx = ""
    dp = Path("data.json"); mp = Path("metric.json")
    if dp.exists():
        ctx += "[data.json]\n" + dp.read_text(encoding="utf-8", errors="ignore") + "\n\n"
    if mp.exists():
        ctx += "[metric.json]\n" + mp.read_text(encoding="utf-8", errors="ignore")
    if not ctx:
        raise RuntimeError("Expected data.json and/or metric.json in the current folder.")
    # keep full context—model will be asked to avoid reading filenames aloud
    return ctx

# ========== Turn Budgeting ==========
TARGET_SECONDS = 120.0
TURNS = 4  # A/B x 4 ≈ ~2 minutes with SSML pauses and 24kHz synthesis

def make_a_ssml(text: str) -> str:
    return to_ssml(strip_markup(text), VOICE_A, style="newscast-casual", rate="-2%", pitch="+0%")

def make_b_ssml(text: str) -> str:
    return to_ssml(strip_markup(text), VOICE_B, style="empathetic", rate="-1%", pitch="-1%")

# ========== Main ==========
OUT_WAV = "podcast_offline_version1.wav"

async def run_podcast():
    context = load_context()
    # We do not print agent lines; we only produce one WAV.
    with wave.open(OUT_WAV, "wb") as master_wf:
        master_wf.setnchannels(1); master_wf.setsampwidth(2); master_wf.setframerate(24000)
        elapsed = 0.0

        # Short, warm intro (1 sentence)
        intro = "Welcome to the quick data briefing—two voices, one dataset, and sharp takeaways in under two minutes."
        ssml = make_a_ssml(intro)
        w = tts_ssml_to_wav(ssml); append_wav_into_master(w, master_wf); elapsed += get_wav_duration_seconds(w); os.remove(w)

        # Seed prompt for A
        last = f"Use these datasets as the sole factual source. Keep conversation style; no lists.\n{context[:9000]}"

        for _ in range(TURNS):
            if elapsed >= TARGET_SECONDS: break

            # Agent A
            a_out = await llm(SYSTEM_A, last, max_tokens=180, temperature=0.55)
            ssml_a = make_a_ssml(a_out)
            wa = tts_ssml_to_wav(ssml_a); append_wav_into_master(wa, master_wf); elapsed += get_wav_duration_seconds(wa); os.remove(wa)
            if elapsed >= TARGET_SECONDS: break

            # Agent B
            b_in = f"Agent A just said: {a_out}\nReference (do not read filenames):\n{context[:7000]}"
            b_out = await llm(SYSTEM_B, b_in, max_tokens=180, temperature=0.55)
            ssml_b = make_b_ssml(b_out)
            wb = tts_ssml_to_wav(ssml_b); append_wav_into_master(wb, master_wf); elapsed += get_wav_duration_seconds(wb); os.remove(wb)

        # Tight closing if room remains
        if elapsed < TARGET_SECONDS - 1.2:
            closing = "That’s our briefing—clear signals, open questions, and concrete next steps. Thanks for listening."
            ssml_c = make_b_ssml(closing)
            wc = tts_ssml_to_wav(ssml_c); append_wav_into_master(wc, master_wf); os.remove(wc)

    print(f"✅ Saved {OUT_WAV}")

if __name__ == "__main__":
    try:
        asyncio.run(run_podcast())
    except Exception as e:
        print(f"❌ Error: {e}")
