# podcast_offline_version1.py — 2-minute audio-only debate (A/B), SSML fixed + fallbacks + transcript
# Azure OpenAI (LLM) + Azure Speech (AAD) — final WAV + TXT transcript
# pip install -U azure-cognitiveservices-speech azure-identity openai python-dotenv

import os, sys, asyncio, wave, tempfile, re
from pathlib import Path
from dotenv import load_dotenv; load_dotenv()

# ========== Azure OpenAI ==========
from openai import AzureOpenAI
AZURE_OPENAI_KEY        = os.getenv("AZURE_OPENAI_KEY") or os.getenv("OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT   = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")
OPENAI_API_VERSION      = os.getenv("OPENAI_API_VERSION", "2024-05-01-preview")
if not all([AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT, OPENAI_API_VERSION]):
    raise RuntimeError("Missing Azure OpenAI env vars")

oai = AzureOpenAI(api_key=AZURE_OPENAI_KEY, azure_endpoint=AZURE_OPENAI_ENDPOINT, api_version=OPENAI_API_VERSION)

def llm_sync(panel_prompt: str, msg: str, max_tokens: int = 260, temperature: float = 0.6) -> str:
    r = oai.chat.completions.create(
        model=AZURE_OPENAI_DEPLOYMENT,
        messages=[{"role":"system","content":panel_prompt},{"role":"user","content":msg}],
        max_tokens=max_tokens, temperature=temperature)
    return (r.choices[0].message.content or "").strip()

async def llm(panel_prompt: str, msg: str, max_tokens: int = 260, temperature: float = 0.6) -> str:
    return await asyncio.to_thread(llm_sync, panel_prompt, msg, max_tokens, temperature)

# ========== Azure Speech (AAD) ==========
import azure.cognitiveservices.speech as speechsdk
from azure.identity import ClientSecretCredential

TENANT_ID     = os.getenv("TENANT_ID")
CLIENT_ID     = os.getenv("CLIENT_ID")
CLIENT_SECRET = os.getenv("CLIENT_SECRET")
SPEECH_REGION = os.getenv("SPEECH_REGION", "eastus")
RESOURCE_ID   = os.getenv("RESOURCE_ID")  # optional
COG_SCOPE     = "https://cognitiveservices.azure.com/.default"
if not all([TENANT_ID, CLIENT_ID, CLIENT_SECRET, SPEECH_REGION]):
    raise RuntimeError("Missing AAD Speech env vars (TENANT_ID, CLIENT_ID, CLIENT_SECRET, SPEECH_REGION)")

cred = ClientSecretCredential(tenant_id=TENANT_ID, client_id=CLIENT_ID, client_secret=CLIENT_SECRET)
def cog_token_str() -> str:
    raw = cred.get_token(COG_SCOPE).token
    return f"aad#{RESOURCE_ID}#{raw}" if RESOURCE_ID else raw

VOICE_A = os.getenv("VOICE_A", "en-US-GuyNeural")   # analyst
VOICE_B = os.getenv("VOICE_B", "en-US-AriaNeural")  # strategist

def ssml_wrapper(inner: str, voice: str, style: str | None, rate: str, pitch: str) -> str:
    if style:
        return f"""<speak version="1.0" xml:lang="en-US"
                 xmlns="http://www.w3.org/2001/10/synthesis"
                 xmlns:mstts="http://www.w3.org/2001/mstts">
  <voice name="{voice}">
    <mstts:express-as style="{style}">
      <prosody rate="{rate}" pitch="{pitch}">
        {inner}
      </prosody>
    </mstts:express-as>
  </voice>
</speak>"""
    else:
        return f"""<speak version="1.0" xml:lang="en-US"
                 xmlns="http://www.w3.org/2001/10/synthesis">
  <voice name="{voice}">
    <prosody rate="{rate}" pitch="{pitch}">
      {inner}
    </prosody>
  </voice>
</speak>"""

def to_ssml(text: str, voice: str, style: str | None, rate: str = "0%", pitch: str = "0%") -> str:
    # Conversational pacing: add short breaks between sentences
    sents = re.split(r'(?<=[.!?])\s+', text.strip())
    parts = []
    for s in sents:
        s = s.strip()
        if s:
            parts.append(f"{s}<break time='220ms'/>")
    inner = " ".join(parts) if parts else text
    return ssml_wrapper(inner, voice, style, rate, pitch)

def tts_ssml_to_wav_with_fallback(text: str, voice: str, style: str | None, rate: str, pitch: str) -> str:
    """
    Try SSML with style -> fallback SSML without style -> fallback plain text.
    Returns path to temp WAV.
    """
    cfg = speechsdk.SpeechConfig(auth_token=cog_token_str(), region=SPEECH_REGION)
    cfg.set_speech_synthesis_output_format(speechsdk.SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm)

    try_styles = [style, None] if style else [None]
    fd, tmp_path = tempfile.mkstemp(prefix="agent_tts_", suffix=".wav"); os.close(fd)

    for sty in try_styles:
        ssml = to_ssml(text, voice, sty, rate, pitch)
        audio_cfg = speechsdk.audio.AudioOutputConfig(filename=tmp_path)
        synth = speechsdk.SpeechSynthesizer(speech_config=cfg, audio_config=audio_cfg)
        res = synth.speak_ssml_async(ssml).get()
        if res.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
            return tmp_path

    # Plain text fallback
    audio_cfg = speechsdk.audio.AudioOutputConfig(filename=tmp_path)
    synth = speechsdk.SpeechSynthesizer(speech_config=cfg, audio_config=audio_cfg)
    res = synth.speak_text_async(text).get()
    if res.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
        return tmp_path

    try: os.remove(tmp_path)
    except Exception: pass
    raise RuntimeError("TTS canceled even after SSML/style fallbacks")

def get_wav_duration_seconds(path: str) -> float:
    with wave.open(path, "rb") as r:
        frames = r.getnframes(); rate = r.getframerate()
        return frames / float(rate) if rate else 0.0

def append_wav_into_master(src_path: str, master_wf: wave.Wave_write):
    with wave.open(src_path, "rb") as r:
        frames = r.readframes(r.getnframes())
        master_wf.writeframes(frames)

def strip_markup(text: str) -> str:
    t = re.sub(r'[`*_#>]+', ' ', text)
    t = re.sub(r'\s{2,}', ' ', t).strip()
    return t

# ========== Prompts (expert, conversational, cover ALL metrics) ==========
SYSTEM_A = """
You are Agent A, a senior data analyst speaking live with Agent B (a strategist).
No lists, no headings, no hashtags, no file names—just natural, crisp conversation.

You have read two datasets: weekly aggregates across 2022–2025 (with YTD and MoM/WoW) and monthly KPIs (ASA in seconds, Average Call Duration in minutes, Claim Processing Time in days, and any other metrics present). In each turn (2–3 short sentences; ~35–55 words), synthesize across ALL available metrics: call out concrete ranges, spikes/drops by month, notable YTD deltas, and how KPIs relate to weekly aggregates. Explain what it means operationally and end with a pointed question to Agent B.
"""

SYSTEM_B = """
You are Agent B, a senior strategist debating live with Agent A.
No lists, no headings, no hashtags, no file names—just natural, crisp conversation.

Listen to A and respond in 2–3 short sentences (~35–55 words). Diagnose plausible drivers (seasonality, staffing, backlog, routing/SLA policy, demand mix) and link weekly aggregates to monthly KPIs across ALL metrics present. Challenge or refine A with evidence and end by proposing a sharp next step or tradeoff back to A.
"""

# ========== Data Loader ==========
def load_context() -> str:
    ctx = ""
    dp = Path("data.json"); mp = Path("metric.json")
    if dp.exists():
        ctx += "[data.json]\n" + dp.read_text(encoding="utf-8", errors="ignore") + "\n\n"
    if mp.exists():
        ctx += "[metric.json]\n" + mp.read_text(encoding="utf-8", errors="ignore")
    if not ctx:
        raise RuntimeError("Expected data.json and/or metric.json in the current folder.")
    return ctx

# ========== Turn Budgeting ==========
TARGET_SECONDS = 120.0
TURNS = 4  # A/B x 4 ≈ ~2 minutes

def make_a_audio(text: str) -> str:
    clean = strip_markup(text)
    return tts_ssml_to_wav_with_fallback(clean, VOICE_A, style="newscast-casual", rate="-2%", pitch="+0%")

def make_b_audio(text: str) -> str:
    clean = strip_markup(text)
    return tts_ssml_to_wav_with_fallback(clean, VOICE_B, style="empathetic", rate="-1%", pitch="-1%")

# ========== Main ==========
OUT_WAV = "podcast_offline_version1.wav"
OUT_TXT = "podcast_offline_version1.txt"

async def run_podcast():
    context = load_context()
    transcript: list[str] = []

    with wave.open(OUT_WAV, "wb") as master_wf:
        master_wf.setnchannels(1); master_wf.setsampwidth(2); master_wf.setframerate(24000)
        elapsed = 0.0

        # Short intro (A)
        intro = "Welcome to the quick data briefing—two voices, one dataset, and sharp takeaways in under two minutes."
        transcript.append(f"Agent A: {strip_markup(intro)}")
        w = make_a_audio(intro); append_wav_into_master(w, master_wf); elapsed += get_wav_duration_seconds(w); os.remove(w)

        seed = f"Use these datasets as the sole factual source. Keep conversation style; no lists.\n{context[:9000]}"

        for _ in range(TURNS):
            if elapsed >= TARGET_SECONDS: break

            # Agent A
            a_out = await llm(SYSTEM_A, seed, max_tokens=180, temperature=0.55)
            transcript.append(f"Agent A: {strip_markup(a_out)}")
            wa = make_a_audio(a_out); append_wav_into_master(wa, master_wf); elapsed += get_wav_duration_seconds(wa); os.remove(wa)
            if elapsed >= TARGET_SECONDS: break

            # Agent B
            b_in = f"Agent A just said: {a_out}\nReference (do not read filenames):\n{context[:7000]}"
            b_out = await llm(SYSTEM_B, b_in, max_tokens=180, temperature=0.55)
            transcript.append(f"Agent B: {strip_markup(b_out)}")
            wb = make_b_audio(b_out); append_wav_into_master(wb, master_wf); elapsed += get_wav_duration_seconds(wb); os.remove(wb)

        # Optional closing if time remains (B)
        if elapsed < TARGET_SECONDS - 1.2:
            closing = "That’s our briefing—clear signals, open questions, and concrete next steps. Thanks for listening."
            transcript.append(f"Agent B: {strip_markup(closing)}")
            wc = make_b_audio(closing); append_wav_into_master(wc, master_wf); os.remove(wc)

    # Save transcript
    with open(OUT_TXT, "w", encoding="utf-8") as f:
        f.write("\n".join(transcript))

    print(f"✅ Saved {OUT_WAV} and {OUT_TXT}")

if __name__ == "__main__":
    try:
        asyncio.run(run_podcast())
    except Exception as e:
        print(f"❌ Error: {e}")
```0
