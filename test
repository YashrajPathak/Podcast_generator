# agent2_demo_livekit_local.py
# pip install livekit-rtc azure-cognitiveservices-speech azure-identity openai python-dotenv
import os, time, asyncio
from dotenv import load_dotenv; load_dotenv()

# --- Azure OpenAI (brain) ---
from openai import AzureOpenAI
AZURE_OPENAI_API_KEY   = os.getenv("AZURE_OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT  = os.getenv("AZURE_OPENAI_ENDPOINT")   # e.g. https://<your>.openai.azure.com/
AZURE_OPENAI_DEPLOYMENT= os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o-mini")  # your deployment name
oai = AzureOpenAI(api_key=AZURE_OPENAI_API_KEY, azure_endpoint=AZURE_OPENAI_ENDPOINT)

# --- Azure Speech (STT + TTS) with AAD ClientSecretCredential (no subscription key) ---
import azure.cognitiveservices.speech as speechsdk
from azure.identity import ClientSecretCredential

TENANT_ID   = os.getenv("AZURE_TENANT_ID")
CLIENT_ID   = os.getenv("AZURE_CLIENT_ID")
CLIENT_SECRET = os.getenv("AZURE_CLIENT_SECRET")
SPEECH_REGION = os.getenv("AZURE_SPEECH_REGION")   # e.g., "eastus"
STT_LANGUAGE  = os.getenv("STT_LANGUAGE", "en-US")

# Acquire AAD token for Cognitive Services
COG_SCOPE = "https://cognitiveservices.azure.com/.default"
cred = ClientSecretCredential(tenant_id=TENANT_ID, client_id=CLIENT_ID, client_secret=CLIENT_SECRET)

def get_speech_auth_token() -> str:
    token = cred.get_token(COG_SCOPE)
    # token.expires_on is epoch seconds; Speech SDK expects raw bearer token string
    return token.token

# Build SpeechConfig using auth token (no subscription key)
speech_cfg = speechsdk.SpeechConfig(region=SPEECH_REGION)
speech_cfg.speech_recognition_language = STT_LANGUAGE
# Initial token set
speech_cfg.set_property(speechsdk.PropertyId.SpeechServiceAuthorization_Token, get_speech_auth_token())
# TTS will output raw PCM we can feed to LiveKit track:
TTS_FORMAT = speechsdk.SpeechSynthesisOutputFormat.Raw24Khz16BitMonoPcm

# Background refresher to keep token valid (Speech tokens typically ~10 minutes)
async def refresh_speech_token_loop():
    while True:
        try:
            speech_cfg.set_property(speechsdk.PropertyId.SpeechServiceAuthorization_Token, get_speech_auth_token())
        except Exception as e:
            print("‚ö†Ô∏è Speech token refresh failed:", e)
        # refresh a bit before expiry; 9 minutes is safe
        await asyncio.sleep(9 * 60)

# --- LiveKit client SDK (demo/local mode; no server, no LK keys) ---
from livekit import rtc
SAMPLE_RATE, CHANNELS, FRAME_MS = 24000, 1, 20
SAMPLES_PER_CH  = SAMPLE_RATE * FRAME_MS // 1000
BYTES_PER_FRAME = SAMPLES_PER_CH * CHANNELS * 2

async def publish_pcm_as_track(room: rtc.Room, identity: str, pcm: bytes):
    """Publish PCM bytes into a LocalAudioTrack and also play them locally."""
    source = rtc.AudioSource(sample_rate=SAMPLE_RATE, num_channels=CHANNELS)
    track  = rtc.LocalAudioTrack.create_audio_track(f"{identity}-voice", source)
    await room.local_participant.publish_track(track)

    # also attach a local player so you can hear it in demo mode
    player = rtc.AudioFramePlayer()
    track.add_sink(player)

    for i in range(0, len(pcm), BYTES_PER_FRAME):
        chunk = pcm[i:i+BYTES_PER_FRAME]
        if len(chunk) < BYTES_PER_FRAME: break
        frame = rtc.AudioFrame(chunk, SAMPLE_RATE, CHANNELS, SAMPLES_PER_CH)
        await source.capture_frame(frame)
        await asyncio.sleep(FRAME_MS / 1000)

async def main():
    # Start token refresh task
    asyncio.create_task(refresh_speech_token_loop())

    # 1) Create a local-only "room" (no server, no keys)
    room = rtc.Room()

    # 2) Agent1 mic track (so you can talk)
    mic = rtc.AudioSource.capture_microphone()
    talker = rtc.LocalAudioTrack.create_audio_track("agent1-mic", mic)
    await room.local_participant.publish_track(talker)

    # Also play back mic locally so you know it‚Äôs capturing
    mic_player = rtc.AudioFramePlayer()
    talker.add_sink(mic_player)
    print("üéô Agent1 mic active. Speak a sentence, I‚Äôll reply.")

    # 3) Azure STT recognizer (uses system mic) ‚Äî uses the same SpeechConfig with AAD token
    rec_audio = speechsdk.audio.AudioConfig(use_default_microphone=True)
    recognizer = speechsdk.SpeechRecognizer(speech_config=speech_cfg, audio_config=rec_audio)

    # 4) On recognized text ‚Üí Azure OpenAI ‚Üí TTS ‚Üí publish via LiveKit track
    def on_recognized(evt):
        if evt.result.reason != speechsdk.ResultReason.RecognizedSpeech:
            return
        user_text = (evt.result.text or "").strip()
        if not user_text:
            return
        print(f"üëÇ Agent1 said: {user_text}")

        # Generate a short reply (Azure OpenAI)
        try:
            resp = oai.chat.completions.create(
                model=AZURE_OPENAI_DEPLOYMENT,
                messages=[
                    {"role": "system", "content": "You are a concise helpful panelist."},
                    {"role": "user", "content": user_text}
                ],
                max_tokens=80, temperature=0.7,
            )
            reply = (resp.choices[0].message.content or "").strip()
        except Exception as e:
            reply = "Sorry, I had an issue thinking about that."
            print("OpenAI error:", e)

        print(f"üß† Agent2 reply: {reply}")

        # Synthesize reply ‚Üí PCM (using auth token speech_cfg)
        try:
            tts_cfg = speechsdk.SpeechConfig(region=SPEECH_REGION)
            tts_cfg.set_property(speechsdk.PropertyId.SpeechServiceAuthorization_Token, get_speech_auth_token())
            tts_cfg.set_speech_synthesis_output_format(TTS_FORMAT)
            tts_syn = speechsdk.SpeechSynthesizer(speech_config=tts_cfg, audio_config=None)
            tts_res = tts_syn.speak_text_async(reply).get()
            if tts_res.reason != speechsdk.ResultReason.SynthesizingAudioCompleted:
                print("TTS failed:", tts_res.reason); return
            pcm = tts_res.audio_data
        except Exception as e:
            print("Azure TTS error:", e); return

        # Publish reply as a LiveKit local track (and play it)
        asyncio.create_task(publish_pcm_as_track(room, "agent2", pcm))

    recognizer.recognized.connect(on_recognized)
    recognizer.start_continuous_recognition()
    print("üéß Listening... (Ctrl+C to stop)")

    # Keep running
    try:
        await asyncio.Future()
    except KeyboardInterrupt:
        pass

if __name__ == "__main__":
    # Required env vars:
    #  AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET, AZURE_SPEECH_REGION
    #  AZURE_OPENAI_API_KEY (or OPENAI_API_KEY), AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT
    asyncio.run(main())
