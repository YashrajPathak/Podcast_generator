# two_agents_livekit_client.py
# Client-side only:
# - If LIVEKIT_URL & LIVEKIT_TOKEN are set: connects, publishes audio frames.
# - If not set: runs fully local and plays audio without LiveKit.
#
# Voice: Azure Speech (AAD token, no speech key)
# Brain: Azure OpenAI
# Features: spoken greeting, paste context, A/B round-robin, parallel "i" interrupt,
#           transcript + WAV export on Ctrl+C

import os, sys, asyncio, wave, io
from dotenv import load_dotenv; load_dotenv()

# ---------------- Azure OpenAI (LLM) ----------------
from openai import AzureOpenAI
AZURE_OPENAI_KEY        = os.getenv("AZURE_OPENAI_KEY") or os.getenv("OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT   = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")
OPENAI_API_VERSION      = os.getenv("OPENAI_API_VERSION", "2024-05-01-preview")
if not all([AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT, OPENAI_API_VERSION]):
    raise RuntimeError("Missing Azure OpenAI env vars")
oai = AzureOpenAI(api_key=AZURE_OPENAI_KEY, azure_endpoint=AZURE_OPENAI_ENDPOINT, api_version=OPENAI_API_VERSION)

# ---------------- Azure Speech (AAD, no speech key) ----------------
import azure.cognitiveservices.speech as speechsdk
from azure.identity import ClientSecretCredential

TENANT_ID     = os.getenv("TENANT_ID")
CLIENT_ID     = os.getenv("CLIENT_ID")
CLIENT_SECRET = os.getenv("CLIENT_SECRET")
SPEECH_REGION = os.getenv("SPEECH_REGION", "eastus")
RESOURCE_ID   = os.getenv("RESOURCE_ID")  # optional
COG_SCOPE     = "https://cognitiveservices.azure.com/.default"
if not all([TENANT_ID, CLIENT_ID, CLIENT_SECRET, SPEECH_REGION]):
    raise RuntimeError("Missing AAD Speech env vars (TENANT_ID, CLIENT_ID, CLIENT_SECRET, SPEECH_REGION)")
cred = ClientSecretCredential(tenant_id=TENANT_ID, client_id=CLIENT_ID, client_secret=CLIENT_SECRET)
def cog_token_str() -> str:
    raw = cred.get_token(COG_SCOPE).token
    return f"aad#{RESOURCE_ID}#{raw}" if RESOURCE_ID else raw

# ---------------- LiveKit Client SDK (optional) ----------------
from livekit import rtc
LIVEKIT_URL   = os.getenv("LIVEKIT_URL", "").strip()
LIVEKIT_TOKEN = os.getenv("LIVEKIT_TOKEN", "").strip()
USE_LIVEKIT   = bool(LIVEKIT_URL and LIVEKIT_TOKEN)

# ---------------- Local audio playback (no server) ----------------
# pydub + simpleaudio for simple playback of our 24kHz mono PCM
from pydub import AudioSegment
from pydub.playback import play

# ---------------- Session / Agent config ----------------
SESSION_ID = os.getenv("SESSION_ID", "demo-session")
AGENT_A    = os.getenv("AGENT_A", "Agent A")
AGENT_B    = os.getenv("AGENT_B", "Agent B")
VOICE_A    = os.getenv("VOICE_A", "en-US-GuyNeural")
VOICE_B    = os.getenv("VOICE_B", "en-US-AriaNeural")

SAMPLE_RATE, CHANNELS, FRAME_MS = 24000, 1, 20
SAMPLES_PER_CH  = SAMPLE_RATE * FRAME_MS // 1000
BYTES_PER_FRAME = SAMPLES_PER_CH * CHANNELS * 2  # 16-bit mono

def track_name(agent: str) -> str:
    return f"{SESSION_ID}-{agent}-voice"

# ---------------- LLM prompts ----------------
SYSTEM_A = ("You are Agent A, a friendly analyst. "
            "Speak 2‚Äì3 short sentences; reference the context; ask a probing question.")
SYSTEM_B = ("You are Agent B, a pragmatic strategist. "
            "Speak 2‚Äì3 short sentences; build on/challenge A respectfully; propose next steps.")

async def llm(panel_prompt: str, msg: str) -> str:
    r = await asyncio.to_thread(lambda: oai.chat.completions.create(
        model=AZURE_OPENAI_DEPLOYMENT,
        messages=[{"role": "system", "content": panel_prompt},
                  {"role": "user",   "content": msg}],
        max_tokens=220, temperature=0.8
    ))
    return (r.choices[0].message.content or "").strip()

# ---------------- LiveKit helpers ----------------
async def lk_connect(room: rtc.Room):
    if not USE_LIVEKIT:
        print("üîå LiveKit: local-only mode (no server connection).")
        return False
    await room.connect(LIVEKIT_URL, LIVEKIT_TOKEN)
    print(f"üîå LiveKit: connected to {LIVEKIT_URL}")
    return True

async def publish_pcm_livekit(room: rtc.Room, agent_label: str, pcm: bytes):
    """
    Publish raw PCM (24kHz, 16-bit mono) as frames into a LiveKit LocalAudioTrack.
    Only call this when actually connected to a LiveKit room.
    """
    source = rtc.AudioSource(sample_rate=SAMPLE_RATE, num_channels=CHANNELS)
    track  = rtc.LocalAudioTrack.create_audio_track(track_name(agent_label), source)
    await room.local_participant.publish_track(track)

    for i in range(0, len(pcm), BYTES_PER_FRAME):
        seg = pcm[i:i+BYTES_PER_FRAME]
        if len(seg) < BYTES_PER_FRAME:
            continue
        frame = rtc.AudioFrame(seg, SAMPLE_RATE, CHANNELS, SAMPLES_PER_CH)
        await source.capture_frame(frame)
        await asyncio.sleep(FRAME_MS / 1000)

# ---------------- TTS: stream -> collect PCM -> (LiveKit or local playback) ----------------
async def tts_get_pcm(text: str, voice: str) -> bytes:
    """
    Synthesizes text with Azure TTS to raw PCM (24kHz 16-bit mono), returns bytes.
    """
    cfg = speechsdk.SpeechConfig(auth_token=cog_token_str(), region=SPEECH_REGION)
    cfg.speech_synthesis_voice_name = voice
    cfg.set_speech_synthesis_output_format(
        speechsdk.SpeechSynthesisOutputFormat.Raw24Khz16BitMonoPcm
    )
    synth = speechsdk.SpeechSynthesizer(speech_config=cfg, audio_config=None)
    res = await asyncio.to_thread(lambda: synth.speak_text_async(text).get())
    if res.reason != speechsdk.ResultReason.SynthesizingAudioCompleted:
        raise RuntimeError(f"TTS failed: {res.reason}")
    stream = speechsdk.AudioDataStream(res)

    # Read the whole stream to bytes (buffered)
    chunks = []
    buf = bytearray(BYTES_PER_FRAME * 20)
    while True:
        n = stream.read_data(buf)
        if n == 0:
            break
        chunks.append(bytes(buf[:n]))
    return b"".join(chunks)

def local_play_pcm(pcm: bytes):
    """
    Local playback for 24kHz 16-bit mono PCM using pydub + simpleaudio.
    """
    seg = AudioSegment(
        data=pcm,
        sample_width=2,
        frame_rate=SAMPLE_RATE,
        channels=1,
    )
    play(seg)

# ---------------- Interrupts ----------------
interrupt_q: asyncio.Queue[str] = asyncio.Queue()
stop_listening = asyncio.Event()

async def read_interrupts():
    """
    Non-blocking keyboard interrupt system:
    - Press a single 'i' + Enter to enter interrupt mode (mic ON simulation).
    - Then type your question/statement and press Enter to submit.
    - It enqueues the text and resumes the debate.
    """
    print("‚è∏  Tip: type 'i' + Enter to interrupt, then type your question and press Enter.")
    loop = asyncio.get_running_loop()
    while not stop_listening.is_set():
        try:
            line = await asyncio.to_thread(sys.stdin.readline)
            if not line:
                await asyncio.sleep(0.05)
                continue
            first = line.strip()
            if first.lower() == "i":
                # prompt for the actual content
                print("üéôÔ∏è  Speak/type your interruption, then Enter:")
                content = await asyncio.to_thread(sys.stdin.readline)
                content = (content or "").strip()
                if content:
                    await interrupt_q.put(content)
                    print("‚û°Ô∏è  Interruption captured.")
            else:
                # ignore random enters while debate is running
                pass
        except Exception:
            await asyncio.sleep(0.05)

# ---------------- Debate runner (round-robin + parallel interrupts) ----------------
async def run_podcast():
    transcript: list[str] = []
    audio_accum: list[bytes] = []

    # LiveKit room (maybe connect)
    room = rtc.Room()
    connected = await lk_connect(room)

    # Background interrupt watcher
    asyncio.create_task(read_interrupts())

    # 1) Spoken greeting
    greet = "Hi Yashraj, how are you? Ready to listen to a debate on any topic or data? Just paste it below."
    print(f"\nüü¶ {AGENT_A} (greeting): {greet}\n")
    pcm = await tts_get_pcm(greet, VOICE_A)
    audio_accum.append(pcm)
    if connected:
        await publish_pcm_livekit(room, AGENT_A, pcm)
    else:
        local_play_pcm(pcm)
    transcript.append(f"{AGENT_A} (greeting): {greet}")

    # 2) Paste context once
    print("üì• Paste your context and press Enter:")
    context = sys.stdin.readline().strip()
    if not context:
        context = "Sample context about your topic."
    print("üìö Context captured. Starting debate...\n")

    # Warm-up (B)
    warm = await llm(SYSTEM_B, "Agent A greeted the user; reply briefly and warmly, then say you're ready.")
    print(f"üü© {AGENT_B} (warm-up): {warm}\n")
    pcm = await tts_get_pcm(warm, VOICE_B)
    audio_accum.append(pcm)
    if connected:
        await publish_pcm_livekit(room, AGENT_B, pcm)
    else:
        local_play_pcm(pcm)
    transcript.append(f"{AGENT_B} (warm-up): {warm}")

    last = f"Context:\n{context}"

    try:
        while True:
            # Check interrupt immediately before A speaks
            intr = None
            try:
                intr = interrupt_q.get_nowait()
            except asyncio.QueueEmpty:
                pass
            if intr:
                last = f"User interruption: {intr}"

            # A turn
            a_out = await llm(SYSTEM_A, last)
            print(f"\nüü¶ {AGENT_A}: {a_out}\n")
            pcm = await tts_get_pcm(a_out, VOICE_A)
            audio_accum.append(pcm)
            if connected:
                await publish_pcm_livekit(room, AGENT_A, pcm)
            else:
                local_play_pcm(pcm)
            transcript.append(f"{AGENT_A}: {a_out}")

            # Check interrupt again before B replies
            intr = None
            try:
                intr = interrupt_q.get_nowait()
            except asyncio.QueueEmpty:
                pass
            b_in = f"{AGENT_A} said: {a_out}\nOriginal context:\n{context[:1500]}"
            if intr:
                b_in += f"\nUser interruption: {intr}"

            # B turn
            b_out = await llm(SYSTEM_B, b_in)
            print(f"\nüü© {AGENT_B}: {b_out}\n")
            pcm = await tts_get_pcm(b_out, VOICE_B)
            audio_accum.append(pcm)
            if connected:
                await publish_pcm_livekit(room, AGENT_B, pcm)
            else:
                local_play_pcm(pcm)
            transcript.append(f"{AGENT_B}: {b_out}")

            last = b_out

    except KeyboardInterrupt:
        pass
    finally:
        stop_listening.set()
        # Save transcript
        with open("podcast.txt", "w", encoding="utf-8") as f:
            f.write("üéôÔ∏è Podcast Transcript\n\n")
            f.write("\n".join(transcript))
        # Save WAV
        pcm_all = b"".join(audio_accum)
        with wave.open("podcast.wav", "wb") as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)
            wf.setframerate(SAMPLE_RATE)
            wf.writeframes(pcm_all)
        if USE_LIVEKIT:
            await room.disconnect()
        print("‚úÖ Saved podcast.wav and podcast.txt")

# ---------------- Main ----------------
if __name__ == "__main__":
    asyncio.run(run_podcast())
