# two_agents_livekit_local.py ‚Äî interrupt-immediate version
# Azure OpenAI (LLM) + Azure Speech (AAD) + optional LiveKit
# pip install -U azure-cognitiveservices-speech azure-identity openai python-dotenv livekit

import os, sys, asyncio, wave, threading, queue, time
from dotenv import load_dotenv; load_dotenv()

# ---------- Azure OpenAI ----------
from openai import AzureOpenAI
AZURE_OPENAI_KEY        = os.getenv("AZURE_OPENAI_KEY") or os.getenv("OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT   = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")
OPENAI_API_VERSION      = os.getenv("OPENAI_API_VERSION", "2024-05-01-preview")
if not all([AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT, OPENAI_API_VERSION]):
    raise RuntimeError("Missing Azure OpenAI env vars")

oai = AzureOpenAI(
    api_key=AZURE_OPENAI_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_version=OPENAI_API_VERSION,
)

def llm_sync(panel_prompt: str, msg: str) -> str:
    r = oai.chat.completions.create(
        model=AZURE_OPENAI_DEPLOYMENT,
        messages=[{"role":"system","content":panel_prompt},
                  {"role":"user","content":msg}],
        max_tokens=220, temperature=0.8)
    return (r.choices[0].message.content or "").strip()

async def llm(panel_prompt: str, msg: str) -> str:
    return await asyncio.to_thread(llm_sync, panel_prompt, msg)

# ---------- Azure Speech (AAD) ----------
import azure.cognitiveservices.speech as speechsdk
from azure.identity import ClientSecretCredential

TENANT_ID     = os.getenv("TENANT_ID")
CLIENT_ID     = os.getenv("CLIENT_ID")
CLIENT_SECRET = os.getenv("CLIENT_SECRET")
SPEECH_REGION = os.getenv("SPEECH_REGION", "eastus")
RESOURCE_ID   = os.getenv("RESOURCE_ID")
COG_SCOPE     = "https://cognitiveservices.azure.com/.default"

if not all([TENANT_ID, CLIENT_ID, CLIENT_SECRET, SPEECH_REGION]):
    raise RuntimeError("Missing AAD Speech env vars")

cred = ClientSecretCredential(tenant_id=TENANT_ID, client_id=CLIENT_ID, client_secret=CLIENT_SECRET)

def cog_token_str() -> str:
    raw = cred.get_token(COG_SCOPE).token
    return f"aad#{RESOURCE_ID}#{raw}" if RESOURCE_ID else raw

def tts_get_pcm(text: str, voice: str) -> bytes:
    cfg = speechsdk.SpeechConfig(auth_token=cog_token_str(), region=SPEECH_REGION)
    cfg.speech_synthesis_voice_name = voice
    cfg.set_speech_synthesis_output_format(
        speechsdk.SpeechSynthesisOutputFormat.Raw24Khz16BitMonoPcm
    )
    synth = speechsdk.SpeechSynthesizer(speech_config=cfg, audio_config=None)
    res = synth.speak_text_async(text).get()
    if res.reason != speechsdk.ResultReason.SynthesizingAudioCompleted:
        raise RuntimeError(f"TTS failed: {res.reason}")
    return res.audio_data

def local_speak(text: str, voice: str):
    cfg = speechsdk.SpeechConfig(auth_token=cog_token_str(), region=SPEECH_REGION)
    cfg.speech_synthesis_voice_name = voice
    out = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)
    syn = speechsdk.SpeechSynthesizer(speech_config=cfg, audio_config=out)
    res = syn.speak_text_async(text).get()
    if res.reason != speechsdk.ResultReason.SynthesizingAudioCompleted:
        print("‚ö†Ô∏è Local speak failed:", res.reason)

# ---------- Optional LiveKit ----------
from livekit import rtc
LIVEKIT_WS    = os.getenv("LIVEKIT_WS", "").strip()
LIVEKIT_TOKEN = os.getenv("LIVEKIT_TOKEN", "").strip()
USE_LIVEKIT   = bool(LIVEKIT_WS and LIVEKIT_TOKEN)

SESSION_ID = os.getenv("SESSION_ID", "demo-session")
AGENT_A, AGENT_B = "Agent A", "Agent B"
VOICE_A, VOICE_B = "en-US-GuyNeural", "en-US-AriaNeural"

SAMPLE_RATE, CHANNELS, SAMPLE_WIDTH = 24000, 1, 2
FRAME_MS = 20
SAMPLES_PER_CH  = SAMPLE_RATE * FRAME_MS // 1000
BYTES_PER_FRAME = SAMPLES_PER_CH * CHANNELS * SAMPLE_WIDTH

async def lk_connect(room: rtc.Room) -> bool:
    if not USE_LIVEKIT:
        print("üîå LiveKit: local-only mode (no server connection).")
        return False
    await room.connect(LIVEKIT_WS, LIVEKIT_TOKEN)
    print(f"üîå LiveKit: connected to {LIVEKIT_WS}")
    return True

def track_name(agent: str) -> str:
    return f"{SESSION_ID}-{agent}-voice"

async def publish_pcm_livekit(room: rtc.Room, agent_label: str, pcm: bytes):
    if not pcm:
        return
    source = rtc.AudioSource(sample_rate=SAMPLE_RATE, num_channels=CHANNELS)
    track  = rtc.LocalAudioTrack.create_audio_track(track_name(agent_label), source)
    await room.local_participant.publish_track(track)
    for i in range(0, len(pcm), BYTES_PER_FRAME):
        seg = pcm[i:i+BYTES_PER_FRAME]
        if len(seg) < BYTES_PER_FRAME:
            break
        frame = rtc.AudioFrame(seg, SAMPLE_RATE, CHANNELS, SAMPLES_PER_CH)
        await source.capture_frame(frame)
        await asyncio.sleep(FRAME_MS/1000)

# ---------- Prompts ----------
SYSTEM_A = ("You are Agent A, a friendly analyst. "
            "Speak 2‚Äì3 short sentences; reference the context; ask a probing question when helpful.")
SYSTEM_B = ("You are Agent B, a pragmatic strategist. "
            "Speak 2‚Äì3 short sentences; build on/challenge A respectfully; propose next steps.")

# ---------- Interrupt system (immediate) ----------
# - Thread reads stdin continuously.
# - Supports:
#    * `i your question here`  -> inject immediately
#    * `i` (alone) then next line becomes the interrupt
# - The loop checks for high-priority interrupts before each agent speaks and
#   again between A and B (skips B if an interrupt arrives during A).

_input_queue = queue.Queue()          # raw stdin lines
interrupt_q: asyncio.Queue[str] = asyncio.Queue()  # async queue for loop
awaiting_inline = False               # arm for "i" then next line
INT_PREFIX = ("i ", "I ")

def _stdin_reader():
    global awaiting_inline
    while True:
        try:
            line = sys.stdin.readline()
            if not line:
                break
            text = line.strip()
            if not text:
                continue
            # inline: "i your message..."
            if text.startswith(INT_PREFIX):
                msg = text[2:].strip()
                if msg:
                    _input_queue.put(("INT", msg))
                continue
            # arming: just "i" or "I"
            if text.lower() == "i":
                _input_queue.put(("ARM", None))
                continue
            # normal line
            _input_queue.put(("TXT", text))
        except Exception:
            break

async def pump_interrupts():
    armed = False
    while True:
        try:
            typ, payload = _input_queue.get_nowait()
            if typ == "INT":
                # immediate interrupt
                await interrupt_q.put(payload)
                print(f"‚è∏Ô∏è  Interrupt queued: {payload}")
            elif typ == "ARM":
                armed = True
                print("üéôÔ∏è  Interrupt mode armed. Type your question and press Enter‚Ä¶")
            elif typ == "TXT":
                if armed:
                    armed = False
                    await interrupt_q.put(payload)
                    print(f"‚è∏Ô∏è  Interrupt queued: {payload}")
                else:
                    # normal text (used for initial context if not set yet)
                    await interrupt_q.put(("CTX:", payload))
            # small yield
            await asyncio.sleep(0)
        except queue.Empty:
            await asyncio.sleep(0.03)

async def next_interrupt_nowait():
    """Fetch one interrupt (that isn't a CTX sentinel), if present."""
    try:
        item = interrupt_q.get_nowait()
        # If it's tagged context ("CTX:", text), return a sentinel to the caller
        if isinstance(item, tuple) and item[0] == "CTX:":
            # push back for context waiter to catch
            await interrupt_q.put(item)
            return None
        if isinstance(item, tuple):
            # ignore unexpected tuples
            return None
        return item
    except asyncio.QueueEmpty:
        return None

async def wait_for_context_blocking():
    """Wait for first context text, either typed normally or as CTX sentinel."""
    # Drain any old CTX first
    while True:
        try:
            item = interrupt_q.get_nowait()
            if isinstance(item, tuple) and item[0] == "CTX:":
                return item[1]
        except asyncio.QueueEmpty:
            break
    # Then wait
    while True:
        item = await interrupt_q.get()
        if isinstance(item, tuple) and item[0] == "CTX:":
            return item[1]
        # ignore actual interrupts here; re-queue them so debate loop will use them
        if isinstance(item, str):
            await interrupt_q.put(item)

# ---------- Main debate ----------
async def run_with_saver():
    buffers: list[bytes] = []
    transcript: list[str] = []

    threading.Thread(target=_stdin_reader, daemon=True).start()
    asyncio.create_task(pump_interrupts())

    room = rtc.Room()
    connected = await lk_connect(room)

    # Greeting (spoken immediately)
    greet = "Hi Yashraj, how are you? Ready to listen to a debate on any topic or data? Just paste it below."
    print(f"\nüü¶ {AGENT_A} (greeting): {greet}\n")
    local_speak(greet, VOICE_A)
    try:
        pcm = tts_get_pcm(greet, VOICE_A)
        buffers.append(pcm)
        if connected:
            await publish_pcm_livekit(room, AGENT_A, pcm)
    except Exception as e:
        print("‚ö†Ô∏è Greeting TTS error:", e)
    transcript.append(f"{AGENT_A}: {greet}")

    # Ask for context (blocking once)
    print("üì• Paste your context and press Enter (or type normally without 'i'):")
    context = await wait_for_context_blocking()
    if not context:
        context = "Sample context about your topic."
    print("üìö Context captured. Starting debate...\n")

    # Warm-up
    warm = await llm(SYSTEM_B, "Agent A greeted the user; reply briefly and warmly, then say you're ready.")
    print(f"üü© {AGENT_B}: {warm}\n")
    local_speak(warm, VOICE_B)
    try:
        pcm = tts_get_pcm(warm, VOICE_B)
        buffers.append(pcm)
        if connected:
            await publish_pcm_livekit(room, AGENT_B, pcm)
    except Exception as e:
        print("‚ö†Ô∏è Warm-up TTS error:", e)
    transcript.append(f"{AGENT_B}: {warm}")

    last = f"Context to discuss:\n{context}"

    try:
        while True:
            # Priority: apply interrupt before A speaks
            intr = await next_interrupt_nowait()
            if intr:
                print(f"üßë‚Äçüí¨ You: {intr}")
                last = f"User interruption: {intr}"

            # A speaks
            a_out = await llm(SYSTEM_A, last)
            print(f"\nüü¶ {AGENT_A}: {a_out}\n")
            local_speak(a_out, VOICE_A)
            try:
                pcm = tts_get_pcm(a_out, VOICE_A)
                buffers.append(pcm)
                if connected:
                    await publish_pcm_livekit(room, AGENT_A, pcm)
            except Exception as e:
                print("‚ö†Ô∏è TTS A error:", e)
            transcript.append(f"{AGENT_A}: {a_out}")

            # Check interrupt again BEFORE B; if present, skip B and handle immediately next loop
            intr = await next_interrupt_nowait()
            if intr:
                print(f"üßë‚Äçüí¨ You: {intr}")
                last = f"User interruption: {intr}"
                # jump to next turn so A responds to your interruption immediately
                continue

            # B speaks
            b_in = f"{AGENT_A} said: {a_out}\nOriginal context (for reference):\n{context[:1500]}"
            b_out = await llm(SYSTEM_B, b_in)
            print(f"\nüü© {AGENT_B}: {b_out}\n")
            local_speak(b_out, VOICE_B)
            try:
                pcm = tts_get_pcm(b_out, VOICE_B)
                buffers.append(pcm)
                if connected:
                    await publish_pcm_livekit(room, AGENT_B, pcm)
            except Exception as e:
                print("‚ö†Ô∏è TTS B error:", e)
            transcript.append(f"{AGENT_B}: {b_out}")

            last = b_out

    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Debate stopped, saving podcast...")

        # Save transcript
        with open("podcast.txt", "w", encoding="utf-8") as f:
            f.write("üéôÔ∏è Podcast Transcript\n\n")
            f.write("\n".join(transcript))

        # Save audio
        pcm_all = b"".join(buffers)
        with wave.open("podcast.wav", "wb") as wf:
            wf.setnchannels(1)
            wf.setsampwidth(SAMPLE_WIDTH)
            wf.setframerate(SAMPLE_RATE)
            wf.writeframes(pcm_all)

        print("‚úÖ Saved podcast.wav and podcast.txt")
        try:
            if connected:
                await room.disconnect()
        except Exception:
            pass

# ---------- Entry ----------
if __name__ == "__main__":
    asyncio.run(run_with_saver())
