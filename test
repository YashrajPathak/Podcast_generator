# two_agents_livekit_local.py ‚Äî interrupt-immediate + PTT + tools+RAG + diarized + barge-in cancel (ADDITIVE)
# Azure OpenAI (LLM) + Azure Speech (AAD) + optional LiveKit
# pip install -U azure-cognitiveservices-speech azure-identity openai python-dotenv livekit

import os, sys, asyncio, wave, threading, queue, time, shutil, glob, datetime, json, difflib
from pathlib import Path
from dotenv import load_dotenv; load_dotenv()

# ---------- Azure OpenAI ----------
from openai import AzureOpenAI
AZURE_OPENAI_KEY        = os.getenv("AZURE_OPENAI_KEY") or os.getenv("OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT   = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")
OPENAI_API_VERSION      = os.getenv("OPENAI_API_VERSION", "2024-05-01-preview")
if not all([AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT, OPENAI_API_VERSION]):
    raise RuntimeError("Missing Azure OpenAI env vars")

oai = AzureOpenAI(
    api_key=AZURE_OPENAI_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_version=OPENAI_API_VERSION,
)

def llm_sync(panel_prompt: str, msg: str) -> str:
    r = oai.chat.completions.create(
        model=AZURE_OPENAI_DEPLOYMENT,
        messages=[{"role":"system","content":panel_prompt},
                  {"role":"user","content":msg}],
        max_tokens=220, temperature=0.8)
    return (r.choices[0].message.content or "").strip()

async def llm(panel_prompt: str, msg: str) -> str:
    return await asyncio.to_thread(llm_sync, panel_prompt, msg)

# ---------- Azure Speech (AAD) ----------
import azure.cognitiveservices.speech as speechsdk
from azure.identity import ClientSecretCredential

TENANT_ID     = os.getenv("TENANT_ID")
CLIENT_ID     = os.getenv("CLIENT_ID")
CLIENT_SECRET = os.getenv("CLIENT_SECRET")
SPEECH_REGION = os.getenv("SPEECH_REGION", "eastus")
RESOURCE_ID   = os.getenv("RESOURCE_ID")
COG_SCOPE     = "https://cognitiveservices.azure.com/.default"
STT_LANGUAGE  = os.getenv("STT_LANGUAGE", "en-US")  # NEW

if not all([TENANT_ID, CLIENT_ID, CLIENT_SECRET, SPEECH_REGION]):
    raise RuntimeError("Missing AAD Speech env vars")

cred = ClientSecretCredential(tenant_id=TENANT_ID, client_id=CLIENT_ID, client_secret=CLIENT_SECRET)

def cog_token_str() -> str:
    raw = cred.get_token(COG_SCOPE).token
    return f"aad#{RESOURCE_ID}#{raw}" if RESOURCE_ID else raw

def tts_get_pcm(text: str, voice: str) -> bytes:
    cfg = speechsdk.SpeechConfig(auth_token=cog_token_str(), region=SPEECH_REGION)
    cfg.speech_synthesis_voice_name = voice
    cfg.set_speech_synthesis_output_format(
        speechsdk.SpeechSynthesisOutputFormat.Raw24Khz16BitMonoPcm
    )
    synth = speechsdk.SpeechSynthesizer(speech_config=cfg, audio_config=None)
    res = synth.speak_text_async(text).get()
    if res.reason != speechsdk.ResultReason.SynthesizingAudioCompleted:
        raise RuntimeError(f"TTS failed: {res.reason}")
    return res.audio_data

# === NEW: local speak with cancel support (barge-in) ===
_current_synth_lock = threading.Lock()
_current_synth: speechsdk.SpeechSynthesizer | None = None

def local_speak(text: str, voice: str):
    global _current_synth
    cfg = speechsdk.SpeechConfig(auth_token=cog_token_str(), region=SPEECH_REGION)
    cfg.speech_synthesis_voice_name = voice
    out = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)
    syn = speechsdk.SpeechSynthesizer(speech_config=cfg, audio_config=out)
    with _current_synth_lock:
        _current_synth = syn
    res = syn.speak_text_async(text).get()
    with _current_synth_lock:
        _current_synth = None
    if res.reason != speechsdk.ResultReason.SynthesizingAudioCompleted:
        print("‚ö†Ô∏è Local speak failed:", res.reason)

def cancel_speaking():
    global _current_synth
    with _current_synth_lock:
        if _current_synth is not None:
            try:
                _current_synth.stop_speaking_async().get()
            except Exception:
                pass

# ---------- Optional LiveKit ----------
from livekit import rtc
LIVEKIT_WS    = os.getenv("LIVEKIT_WS", "").strip()
LIVEKIT_TOKEN = os.getenv("LIVEKIT_TOKEN", "").strip()
USE_LIVEKIT   = bool(LIVEKIT_WS and LIVEKIT_TOKEN)

SESSION_ID = os.getenv("SESSION_ID", "demo-session")
AGENT_A, AGENT_B = "Agent A", "Agent B"
VOICE_A, VOICE_B = "en-US-GuyNeural", "en-US-AriaNeural"

SAMPLE_RATE, CHANNELS, SAMPLE_WIDTH = 24000, 1, 2
FRAME_MS = 20
SAMPLES_PER_CH  = SAMPLE_RATE * FRAME_MS // 1000
BYTES_PER_FRAME = SAMPLES_PER_CH * CHANNELS * SAMPLE_WIDTH

async def lk_connect(room: rtc.Room) -> bool:
    if not USE_LIVEKIT:
        print("üîå LiveKit: local-only mode (no server connection).")
        return False
    await room.connect(LIVEKIT_WS, LIVEKIT_TOKEN)
    print(f"üîå LiveKit: connected to {LIVEKIT_WS}")
    return True

def track_name(agent: str) -> str:
    return f"{SESSION_ID}-{agent}-voice"

async def publish_pcm_livekit(room: rtc.Room, agent_label: str, pcm: bytes):
    if not pcm:
        return
    source = rtc.AudioSource(sample_rate=SAMPLE_RATE, num_channels=CHANNELS)
    track  = rtc.LocalAudioTrack.create_audio_track(track_name(agent_label), source)
    await room.local_participant.publish_track(track)
    for i in range(0, len(pcm), BYTES_PER_FRAME):
        seg = pcm[i:i+BYTES_PER_FRAME]
        if len(seg) < BYTES_PER_FRAME:
            break
        frame = rtc.AudioFrame(seg, SAMPLE_RATE, CHANNELS, SAMPLES_PER_CH)
        await source.capture_frame(frame)
        await asyncio.sleep(FRAME_MS/1000)

# ---------- Prompts ----------
SYSTEM_A = ("You are Agent A, a friendly analyst. "
            "Speak 2‚Äì3 short sentences; reference the context; ask a probing question when helpful.")
SYSTEM_B = ("You are Agent B, a pragmatic strategist. "
            "Speak 2‚Äì3 short sentences; build on/challenge A respectfully; propose next steps.")

# ---------- Interrupt system (immediate) ----------
# - Thread reads stdin continuously.
# - Supports:
#    * `i your question here`  -> inject immediately
#    * `i` (alone) then speak (PTT) OR type next line (fallback)
#    * `r` -> refresh background inbox immediately

_input_queue = queue.Queue()          # raw stdin lines
interrupt_q: asyncio.Queue[str] = asyncio.Queue()  # async user interrupts
admin_q: asyncio.Queue[str] = asyncio.Queue()      # admin signals like REFRESH
awaiting_inline = False
INT_PREFIX = ("i ", "I ")

def _stdin_reader():
    global awaiting_inline
    while True:
        try:
            line = sys.stdin.readline()
            if not line:
                break
            text = line.strip()
            if not text:
                continue
            if text.startswith(INT_PREFIX):
                msg = text[2:].strip()
                if msg:
                    _input_queue.put(("INT", msg))
                continue
            if text.lower() == "i":
                _input_queue.put(("ARM", None))
                continue
            if text.lower() == "r":
                _input_queue.put(("REFRESH", None))
                continue
            _input_queue.put(("TXT", text))
        except Exception:
            break

async def _ptt_capture_once() -> str | None:
    try:
        cfg = speechsdk.SpeechConfig(auth_token=cog_token_str(), region=SPEECH_REGION)
        cfg.speech_recognition_language = STT_LANGUAGE
        cfg.set_property(speechsdk.PropertyId.SpeechServiceConnection_InitialSilenceTimeoutMs, "2000")
        cfg.set_property(speechsdk.PropertyId.SpeechServiceConnection_EndSilenceTimeoutMs, "500")
        audio = speechsdk.audio.AudioConfig(use_default_microphone=True)
        rec  = speechsdk.SpeechRecognizer(speech_config=cfg, audio_config=audio)

        final_text = {"t": None}
        def _on_recognizing(evt):
            txt = (evt.result.text or "").strip()
            if txt:
                print(f"üéß partial: {txt[:80]}{'‚Ä¶' if len(txt)>80 else ''}")
        def _on_recognized(evt):
            if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:
                final_text["t"] = (evt.result.text or "").strip()
                print(f"‚úÖ final: {final_text['t']}")
        def _on_canceled(evt):
            print(f"‚ö†Ô∏è STT canceled: {evt.reason} {getattr(evt, 'error_details', '')}")

        rec.recognizing.connect(_on_recognizing)
        rec.recognized.connect(_on_recognized)
        rec.canceled.connect(_on_canceled)

        print("üéôÔ∏è Listening‚Ä¶ (speak now)")
        rec.start_continuous_recognition_async().get()

        t0 = time.time()
        while time.time() - t0 < 12:
            if final_text["t"]:
                break
            time.sleep(0.05)

        rec.stop_continuous_recognition_async().get()
        return final_text["t"]
    except Exception as e:
        print("‚ö†Ô∏è STT error:", e)
        return None

async def _ptt_and_enqueue():
    text = await _ptt_capture_once()
    if text:
        if len(text) < 2:
            print("ü´• Ignored very short audio.")
            return
        await interrupt_q.put(text)
        print(f"‚è∏Ô∏è  Interrupt (voice) queued: {text}")
    else:
        print("ü´§ No speech recognized (timeout or silence).")

async def pump_interrupts():
    armed = False
    while True:
        try:
            typ, payload = _input_queue.get_nowait()
            if typ == "INT":
                cancel_speaking()  # NEW: barge-in cancel
                await interrupt_q.put(payload)
                print(f"‚è∏Ô∏è  Interrupt queued: {payload}")
            elif typ == "ARM":
                cancel_speaking()  # NEW: barge-in cancel
                armed = True
                print("üéôÔ∏è  Interrupt mode armed. Speak now (PTT) or type your question‚Ä¶")
                asyncio.create_task(_ptt_and_enqueue())
            elif typ == "REFRESH":
                await admin_q.put("REFRESH")
                print("üîÑ Background refresh requested.")
            elif typ == "TXT":
                if armed:
                    armed = False
                    cancel_speaking()  # NEW: barge-in cancel
                    await interrupt_q.put(payload)
                    print(f"‚è∏Ô∏è  Interrupt queued: {payload}")
                else:
                    await interrupt_q.put(("CTX:", payload))
            await asyncio.sleep(0)
        except queue.Empty:
            await asyncio.sleep(0.03)

async def next_interrupt_nowait():
    try:
        item = interrupt_q.get_nowait()
        if isinstance(item, tuple) and item[0] == "CTX:":
            await interrupt_q.put(item)
            return None
        if isinstance(item, tuple):
            return None
        return item
    except asyncio.QueueEmpty:
        return None

async def wait_for_context_blocking():
    while True:
        try:
            item = interrupt_q.get_nowait()
            if isinstance(item, tuple) and item[0] == "CTX:":
                return item[1]
            if isinstance(item, str):
                return item
        except asyncio.QueueEmpty:
            break
    while True:
        item = await interrupt_q.get()
        if isinstance(item, tuple) and item[0] == "CTX:":
            return item[1]
        if isinstance(item, str):
            return item

# ---------- Background inbox (deterministic, no watcher) ----------
BACKGROUND_DIR = os.getenv("BACKGROUND_DIR", "background")
PROCESSED_SUB  = "_processed"

def _ensure_dirs():
    try:
        os.makedirs(BACKGROUND_DIR, exist_ok=True)
        os.makedirs(os.path.join(BACKGROUND_DIR, PROCESSED_SUB), exist_ok=True)
    except Exception as e:
        print("‚ö†Ô∏è Could not create background dirs:", e)

def ingest_background(bg_chunks: list[str]) -> list[str]:
    _ensure_dirs()
    patterns = [os.path.join(BACKGROUND_DIR, "*.txt"),
                os.path.join(BACKGROUND_DIR, "*.md"),
                os.path.join(BACKGROUND_DIR, "*.json")]
    files = []
    for p in patterns:
        files.extend(glob.glob(p))
    files = [f for f in files if os.path.isfile(f)]
    if not files:
        return []
    processed_names = []
    for fp in sorted(files):
        try:
            with open(fp, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read().strip()
            ts = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            chunk = f"\n\n[Background file: {os.path.basename(fp)} @ {ts}]\n{content}\n"
            bg_chunks.append(chunk)
            dest = os.path.join(BACKGROUND_DIR, PROCESSED_SUB, os.path.basename(fp))
            if os.path.exists(dest):
                base, ext = os.path.splitext(dest); n = 1
                while os.path.exists(f"{base}_{n}{ext}"):
                    n += 1
                dest = f"{base}_{n}{ext}"
            shutil.move(fp, dest)
            processed_names.append(os.path.basename(fp))
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to ingest {fp}: {e}")
    return processed_names

# ---------- File picker (voice/typed) ----------
ALLOWED_EXTS = {".txt", ".md", ".json", ".csv", ".yaml", ".yml"}

def _list_candidate_files(base_dir: Path) -> list[Path]:
    files = []
    for p in sorted(base_dir.iterdir(), key=lambda x: x.name.lower()):
        if p.is_file() and p.suffix.lower() in ALLOWED_EXTS:
            files.append(p)
    return files

def _speak_menu(files: list[Path]) -> str:
    items = [f"{idx+1}: {f.name}" for idx, f in enumerate(files[:10])]
    menu_line = "; ".join(items) if items else "No files found."
    return f"I found {len(files)} files. {menu_line}. Say the name or the number."

async def _read_choice_from_user(timeout_sec: float = 20.0) -> str | None:
    try:
        item = interrupt_q.get_nowait()
        if isinstance(item, tuple) and item[0] == "CTX:":
            return item[1]
        if isinstance(item, str):
            return item
    except asyncio.QueueEmpty:
        pass
    end = time.time() + timeout_sec
    while time.time() < end:
        try:
            item = interrupt_q.get_nowait()
            if isinstance(item, tuple) and item[0] == "CTX:":
                return item[1]
            if isinstance(item, str):
                return item
        except asyncio.QueueEmpty:
            pass
        await asyncio.sleep(0.1)
    return None

def _best_file_match(spoken_or_typed: str, files: list[Path]) -> Path | None:
    s = spoken_or_typed.strip().lower()
    if not s or not files:
        return None
    if s.isdigit():
        i = int(s) - 1
        if 0 <= i < len(files):
            return files[i]
    names = [f.stem.lower() for f in files]
    for f, stem in zip(files, names):
        if s in stem or s == f.name.lower():
            return f
    candidates = [f.name for f in files]
    got = difflib.get_close_matches(spoken_or_typed, candidates, n=1, cutoff=0.4)
    if got:
        name = got[0]
        for f in files:
            if f.name == name:
                return f
    return None

async def pick_file_context_or_none(base_dir: Path) -> tuple[str | None, Path | None]:
    files = _list_candidate_files(base_dir)
    if not files:
        print("üìÇ No candidate files in folder (txt/md/json/csv/yaml).")
        return None, None
    menu_line = _speak_menu(files)
    print(f"üìÇ Files:\n  " + "\n  ".join(f"{i+1}. {p.name}" for i, p in enumerate(files)))
    print("üéôÔ∏è  Say the name/number with PTT (press 'i' then speak), or type it and press Enter.")
    try:
        local_speak(menu_line, VOICE_A)
    except Exception:
        pass
    choice = await _read_choice_from_user(timeout_sec=20.0)
    if not choice:
        print("‚è≥ No choice heard/typed. Skipping file pick.")
        return None, None
    picked = _best_file_match(choice, files)
    if not picked:
        print(f"‚ùì Couldn't match '{choice}' to a file. Skipping file pick.")
        return None, None
    try:
        text = picked.read_text(encoding="utf-8", errors="ignore")
        print(f"‚úÖ Using file: {picked.name}  ({len(text)} chars)")
        return text, picked
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to read {picked}: {e}")
        return None, None

# ---------- RAG index (very simple keyword scoring; no extra deps) ----------
RAG_DIR = Path(os.getenv("RAG_DIR", "rag")).resolve()
RAG_MAX_FILES = int(os.getenv("RAG_MAX_FILES", "200"))
RAG_SNIPPET_CHARS = 280

def _load_rag_corpus() -> list[tuple[Path, str]]:
    corpus = []
    if not RAG_DIR.exists():
        return corpus
    for p in sorted(RAG_DIR.rglob("*")):
        if len(corpus) >= RAG_MAX_FILES: break
        if p.is_file() and p.suffix.lower() in {".txt",".md",".json",".csv",".yaml",".yml"}:
            try:
                corpus.append((p, p.read_text(encoding="utf-8", errors="ignore")))
            except Exception:
                pass
    return corpus

def _score_doc(text: str, query: str) -> int:
    # dumb bag-of-words intersection score
    q = [w.lower() for w in query.split() if len(w) > 2]
    t = text.lower()
    return sum(t.count(w) for w in q)

def rag_search(query: str, top_k: int = 4) -> list[dict]:
    corpus = _load_rag_corpus()
    scored = []
    for p, txt in corpus:
        s = _score_doc(txt, query)
        if s > 0:
            scored.append((s, p, txt))
    scored.sort(key=lambda x: x[0], reverse=True)
    out = []
    for s, p, txt in scored[:top_k]:
        # take first matching window
        q = query.split()
        idx = 0
        for w in q:
            pos = txt.lower().find(w.lower())
            if pos >= 0:
                idx = pos; break
        start = max(0, idx - RAG_SNIPPET_CHARS//2)
        end   = min(len(txt), start + RAG_SNIPPET_CHARS)
        snippet = txt[start:end].replace("\n"," ").strip()
        out.append({"path": str(p), "score": int(s), "snippet": snippet})
    return out

# ---------- Tool registry + function-calling ----------
# We keep your llm() untouched; these are new helpers for tool-aware replies when needed.

NOTES_FILE = Path(os.getenv("NOTES_FILE", "notes.txt")).resolve()

def tool_search_docs(query: str) -> dict:
    results = rag_search(query, top_k=5)
    return {"results": results}

def tool_mark_in_data(query: str) -> dict:
    # Scan RAG corpus and return file + line numbers where query occurs
    hits = []
    corpus = _load_rag_corpus()
    ql = query.lower()
    for p, txt in corpus:
        lines = txt.splitlines()
        for i, line in enumerate(lines, start=1):
            if ql in line.lower():
                hits.append({"path": str(p), "line": i, "text": line.strip()})
    return {"matches": hits[:100]}

def tool_append_note(text: str) -> dict:
    try:
        ts = datetime.datetime.now().isoformat()
        with NOTES_FILE.open("a", encoding="utf-8") as f:
            f.write(f"[{ts}] {text}\n")
        return {"ok": True, "path": str(NOTES_FILE)}
    except Exception as e:
        return {"ok": False, "error": str(e)}

def safe_calc(expr: str) -> dict:
    # extremely restricted eval (numbers + +-*/().)
    allowed = set("0123456789+-*/(). %")
    if not set(expr) <= allowed:
        return {"error": "disallowed characters"}
    try:
        val = eval(expr, {"__builtins__": {}}, {})
        return {"result": val}
    except Exception as e:
        return {"error": str(e)}

TOOLS_SCHEMA = [
    {
        "type": "function",
        "function": {
            "name": "search_docs",
            "description": "Search local document corpus for relevant snippets.",
            "parameters": {
                "type": "object",
                "properties": { "query": {"type":"string"} },
                "required": ["query"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "mark_in_data",
            "description": "Find exact lines in local corpus that contain the query.",
            "parameters": {
                "type": "object",
                "properties": { "query": {"type":"string"} },
                "required": ["query"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "append_note",
            "description": "Append a note to a local notes.txt file.",
            "parameters": {
                "type": "object",
                "properties": { "text": {"type":"string"} },
                "required": ["text"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "math_calc",
            "description": "Do a quick arithmetic calculation.",
            "parameters": {
                "type": "object",
                "properties": { "expr": {"type":"string"} },
                "required": ["expr"]
            }
        }
    },
]

def _call_tool(name: str, args: dict) -> dict:
    if name == "search_docs":
        return tool_search_docs(args.get("query",""))
    if name == "mark_in_data":
        return tool_mark_in_data(args.get("query",""))
    if name == "append_note":
        return tool_append_note(args.get("text",""))
    if name == "math_calc":
        return safe_calc(args.get("expr",""))
    return {"error": f"unknown tool {name}"}

def llm_tool_sync(panel_prompt: str, msg: str) -> str:
    # 1st call: allow tools
    r = oai.chat.completions.create(
        model=AZURE_OPENAI_DEPLOYMENT,
        messages=[{"role":"system","content":panel_prompt},
                  {"role":"user","content":msg}],
        tools=TOOLS_SCHEMA,
        tool_choice="auto",
        max_tokens=300, temperature=0.7
    )
    choice = r.choices[0]
    tool_calls = getattr(choice.message, "tool_calls", None)
    if tool_calls:
        outputs = []
        for tc in tool_calls:
            fn = tc.function
            name = fn.name
            try:
                args = json.loads(fn.arguments or "{}")
            except Exception:
                args = {}
            result = _call_tool(name, args)
            outputs.append({
                "tool_call_id": tc.id,
                "name": name,
                "content": json.dumps(result, ensure_ascii=False)
            })
        # 2nd call: return assistant reasoning with tool results
        msgs = [
            {"role":"system","content":panel_prompt},
            {"role":"user","content":msg},
        ]
        for out in outputs:
            msgs.append({
                "role": "tool",
                "tool_call_id": out["tool_call_id"],
                "name": out["name"],
                "content": out["content"]
            })
        r2 = oai.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=msgs,
            max_tokens=350, temperature=0.7
        )
        return (r2.choices[0].message.content or "").strip()
    else:
        return (choice.message.content or "").strip()

async def llm_tool(panel_prompt: str, msg: str) -> str:
    return await asyncio.to_thread(llm_tool_sync, panel_prompt, msg)

# ---------- Diarized transcript helpers ----------
TRANSCRIPT_JSONL = Path("podcast.jsonl").resolve()

def _log_entry(role: str, text: str):
    entry = {
        "ts": datetime.datetime.now().isoformat(),
        "role": role,
        "text": text
    }
    with open(TRANSCRIPT_JSONL, "a", encoding="utf-8") as f:
        f.write(json.dumps(entry, ensure_ascii=False) + "\n")

# ---------- Main debate ----------
async def run_with_saver():
    buffers: list[bytes] = []
    transcript: list[str] = []
    bg_chunks: list[str] = []

    threading.Thread(target=_stdin_reader, daemon=True).start()
    asyncio.create_task(pump_interrupts())

    room = rtc.Room()
    connected = await lk_connect(room)

    # Greeting (spoken immediately)
    greet = "Hi Yashraj, how are you? Ready to listen to a debate on any topic or data? Just paste it below."
    print(f"\nüü¶ {AGENT_A} (greeting): {greet}\n")
    _log_entry(AGENT_A, greet)
    local_speak(greet, VOICE_A)
    try:
        pcm = tts_get_pcm(greet, VOICE_A)
        buffers.append(pcm)
        if connected:
            await publish_pcm_livekit(room, AGENT_A, pcm)
    except Exception as e:
        print("‚ö†Ô∏è Greeting TTS error:", e)
    transcript.append(f"{AGENT_A}: {greet}")

    # Offer file-pick first
    base_dir = Path(".").resolve()
    file_ctx, file_path = await pick_file_context_or_none(base_dir)
    if file_ctx:
        context_base = file_ctx
        print("üìö Context taken from file. Starting debate...\n")
    else:
        print("üì• Paste your context and press Enter (or press 'i' then speak):")
        context_base = await wait_for_context_blocking()
        if not context_base:
            context_base = "Sample context about your topic."
        print("üìö Context captured. Starting debate...\n")

    # Warm-up
    warm = await llm(SYSTEM_B, "Agent A greeted the user; reply briefly and warmly, then say you're ready.")
    print(f"üü© {AGENT_B}: {warm}\n")
    _log_entry(AGENT_B, warm)
    local_speak(warm, VOICE_B)
    try:
        pcm = tts_get_pcm(warm, VOICE_B)
        buffers.append(pcm)
        if connected:
            await publish_pcm_livekit(room, AGENT_B, pcm)
    except Exception as e:
        print("‚ö†Ô∏è Warm-up TTS error:", e)
    transcript.append(f"{AGENT_B}: {warm}")

    last = f"Context to discuss:\n{context_base}"

    try:
        while True:
            # Admin: refresh background if requested
            try:
                sig = admin_q.get_nowait()
                if sig == "REFRESH":
                    processed = ingest_background(bg_chunks)
                    if processed:
                        print(f"üì• Ingested {len(processed)} background file(s): {processed}")
                        transcript.append(f"[Background ingested: {', '.join(processed)}]")
                        _log_entry("system", f"background_ingested: {processed}")
                    else:
                        print("üì≠ No new background files found.")
            except asyncio.QueueEmpty:
                pass

            # Auto-ingest at A-turn boundary
            processed = ingest_background(bg_chunks)
            if processed:
                print(f"üì• Ingested {len(processed)} background file(s): {processed}")
                transcript.append(f"[Background ingested: {', '.join(processed)}]")
                _log_entry("system", f"background_ingested: {processed}")

            # Build compact tail of background context
            ctx_tail = ""
            if bg_chunks:
                tail = "".join(bg_chunks[-3:])
                ctx_tail = tail[-2000:] if len(tail) > 2000 else tail

            # Priority: apply interrupt before A
            intr = await next_interrupt_nowait()
            if intr:
                print(f"üßë‚Äçüí¨ You: {intr}")
                _log_entry("User", intr)
                last = f"User interruption: {intr}"

            # Agent A (tool-aware)
            a_in = last + (f"\n\n(Reference docs)\n{ctx_tail}" if ctx_tail else "")
            a_out = await llm_tool(SYSTEM_A, a_in)
            print(f"\nüü¶ {AGENT_A}: {a_out}\n")
            _log_entry(AGENT_A, a_out)
            local_speak(a_out, VOICE_A)
            try:
                pcm = tts_get_pcm(a_out, VOICE_A)
                buffers.append(pcm)
                if connected:
                    await publish_pcm_livekit(room, AGENT_A, pcm)
            except Exception as e:
                print("‚ö†Ô∏è TTS A error:", e)
            transcript.append(f"{AGENT_A}: {a_out}")

            # Check interrupt again BEFORE B; if present, skip B
            intr = await next_interrupt_nowait()
            if intr:
                print(f"üßë‚Äçüí¨ You: {intr}")
                _log_entry("User", intr)
                last = f"User interruption: {intr}"
                continue

            # Agent B (tool-aware)
            b_in = f"{AGENT_A} said: {a_out}\nOriginal context (for reference):\n{context_base[:1500]}"
            if ctx_tail:
                b_in += f"\n\n(Reference docs)\n{ctx_tail}"
            b_out = await llm_tool(SYSTEM_B, b_in)
            print(f"\nüü© {AGENT_B}: {b_out}\n")
            _log_entry(AGENT_B, b_out)
            local_speak(b_out, VOICE_B)
            try:
                pcm = tts_get_pcm(b_out, VOICE_B)
                buffers.append(pcm)
                if connected:
                    await publish_pcm_livekit(room, AGENT_B, pcm)
            except Exception as e:
                print("‚ö†Ô∏è TTS B error:", e)
            transcript.append(f"{AGENT_B}: {b_out}")

            last = b_out

    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Debate stopped, saving podcast...")

        with open("podcast.txt", "w", encoding="utf-8") as f:
            f.write("üéôÔ∏è Podcast Transcript\n\n")
            f.write("\n".join(transcript))

        pcm_all = b"".join(buffers)
        with wave.open("podcast.wav", "wb") as wf:
            wf.setnchannels(1)
            wf.setsampwidth(SAMPLE_WIDTH)
            wf.setframerate(SAMPLE_RATE)
            wf.writeframes(pcm_all)

        print("‚úÖ Saved podcast.wav, podcast.txt, and podcast.jsonl")
        try:
            if connected:
                await room.disconnect()
        except Exception:
            pass

# ---------- Entry ----------
if __name__ == "__main__":
    asyncio.run(run_with_saver())
