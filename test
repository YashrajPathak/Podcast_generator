# two_agents_livekit_local.py ‚Äî self-contained (Windows terminal)
# Azure OpenAI (LLM) + Azure Speech (AAD, no speech key) + local audio playback + hotkey voice interrupt
# pip install -U azure-cognitiveservices-speech azure-identity openai python-dotenv

import os, sys, asyncio, wave, tempfile, time, platform
from dotenv import load_dotenv; load_dotenv()

# ---------- Azure OpenAI ----------
from openai import AzureOpenAI
AZURE_OPENAI_KEY        = os.getenv("AZURE_OPENAI_KEY") or os.getenv("OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT   = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")
OPENAI_API_VERSION      = os.getenv("OPENAI_API_VERSION", "2024-05-01-preview")
if not all([AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT, OPENAI_API_VERSION]):
    raise RuntimeError("Missing Azure OpenAI env vars")
oai = AzureOpenAI(api_key=AZURE_OPENAI_KEY, azure_endpoint=AZURE_OPENAI_ENDPOINT, api_version=OPENAI_API_VERSION)

# ---------- Azure Speech (AAD) ----------
import azure.cognitiveservices.speech as speechsdk
from azure.identity import ClientSecretCredential
TENANT_ID     = os.getenv("TENANT_ID")
CLIENT_ID     = os.getenv("CLIENT_ID")
CLIENT_SECRET = os.getenv("CLIENT_SECRET")
SPEECH_REGION = os.getenv("SPEECH_REGION", "eastus")
RESOURCE_ID   = os.getenv("RESOURCE_ID")  # optional
COG_SCOPE     = "https://cognitiveservices.azure.com/.default"
if not all([TENANT_ID, CLIENT_ID, CLIENT_SECRET, SPEECH_REGION]):
    raise RuntimeError("Missing AAD Speech env vars (TENANT_ID, CLIENT_ID, CLIENT_SECRET, SPEECH_REGION)")
cred = ClientSecretCredential(tenant_id=TENANT_ID, client_id=CLIENT_ID, client_secret=CLIENT_SECRET)

def cog_token_str() -> str:
    raw = cred.get_token(COG_SCOPE).token
    return f"aad#{RESOURCE_ID}#{raw}" if RESOURCE_ID else raw

STT_LANGUAGE = os.getenv("STT_LANGUAGE", "en-US")  # e.g., "en-IN" for Indian English

# ---------- LiveKit (import only; no server used) ----------
try:
    from livekit import rtc
    print("üîå LiveKit: local-only (no server).")
except Exception:
    rtc = None
    print("‚ÑπÔ∏è LiveKit import skipped (not required for local playback).")

# ---------- Playback (Windows) ----------
if platform.system().lower() != "windows":
    raise RuntimeError("This script uses winsound (Windows only). For macOS/Linux, replace winsound playback with simpleaudio/pyaudio.")
import winsound

# ---------- Globals ----------
SESSION_ID = "demo-session"
AGENT_A    = "Agent A"
AGENT_B    = "Agent B"
VOICE_A    = "en-US-GuyNeural"
VOICE_B    = "en-US-AriaNeural"

WAV_RATE   = 24000
WAV_CH     = 1
WAV_SW     = 2  # 16-bit

# Debate style
SYSTEM_A = ("You are Agent A, a friendly analyst. "
            "Speak 2‚Äì3 short sentences; reference the context; ask a probing question.")
SYSTEM_B = ("You are Agent B, a pragmatic strategist. "
            "Speak 2‚Äì3 short sentences; build on/challenge A respectfully; propose next steps.")

async def llm(panel_prompt: str, msg: str) -> str:
    r = await asyncio.to_thread(lambda:
        oai.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[{"role": "system", "content": panel_prompt},
                      {"role": "user", "content": msg}],
            max_tokens=220,
            temperature=0.8
        )
    )
    return (r.choices[0].message.content or "").strip()

# ---------- TTS helpers ----------
def tts_to_temp_wav(text: str, voice: str) -> str:
    cfg = speechsdk.SpeechConfig(auth_token=cog_token_str(), region=SPEECH_REGION)
    cfg.speech_synthesis_voice_name = voice
    cfg.set_speech_synthesis_output_format(speechsdk.SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm)
    fd, tmp_path = tempfile.mkstemp(prefix="agent_tts_", suffix=".wav")
    os.close(fd)
    synth = speechsdk.SpeechSynthesizer(
        speech_config=cfg,
        audio_config=speechsdk.audio.AudioOutputConfig(filename=tmp_path)
    )
    res = synth.speak_text_async(text).get()
    if res.reason != speechsdk.ResultReason.SynthesizingAudioCompleted:
        try: os.remove(tmp_path)
        except Exception: pass
        raise RuntimeError(f"TTS failed: {res.reason}")
    return tmp_path

def play_wav_blocking(path: str):
    winsound.PlaySound(path, winsound.SND_FILENAME)

def append_wav_into_master(src_path: str, master_wf: wave.Wave_write):
    with wave.open(src_path, "rb") as r:
        frames = r.readframes(r.getnframes())
        master_wf.writeframes(frames)

# ---------- Interrupt machinery ----------
interrupt_q: asyncio.Queue[str] = asyncio.Queue()
mic_lock = asyncio.Lock()        # ensure only one mic capture at a time
stop_for_interrupt = asyncio.Event()  # signal main loop to pause at turn boundaries

# Hotkey watcher (Windows console): press 'i' to capture one utterance from mic
async def hotkey_watcher():
    import msvcrt  # Windows-only
    print("‚èØ  Press 'i' any time to interrupt and speak. Press Ctrl+C to stop the podcast.")
    while True:
        await asyncio.sleep(0.03)
        if msvcrt.kbhit():
            ch = msvcrt.getch()
            if not ch:
                continue
            try:
                c = ch.decode(errors="ignore").lower()
            except Exception:
                c = ""
            if c == 'i':
                # request immediate pause at next safe point
                stop_for_interrupt.set()
                # launch a mic capture (do not block this watcher)
                asyncio.create_task(capture_one_utterance_and_enqueue())

async def capture_one_utterance_and_enqueue():
    async with mic_lock:
        try:
            print("\nüéô  Listening... (speak one sentence)\n")
            cfg = speechsdk.SpeechConfig(auth_token=cog_token_str(), region=SPEECH_REGION)
            cfg.speech_recognition_language = STT_LANGUAGE
            audio = speechsdk.audio.AudioConfig(use_default_microphone=True)
            rec = speechsdk.SpeechRecognizer(speech_config=cfg, audio_config=audio)

            # single-shot recognition
            res = await asyncio.to_thread(lambda: rec.recognize_once_async().get())
            if res.reason == speechsdk.ResultReason.RecognizedSpeech:
                text = (res.text or "").strip()
                if text:
                    print(f"üëÇ User: {text}")
                    await interrupt_q.put(text)
                else:
                    print("‚Ä¶heard silence.")
            elif res.reason == speechsdk.ResultReason.NoMatch:
                print("‚Ä¶couldn‚Äôt understand (NoMatch).")
            else:
                # Canceled or error
                try:
                    print(f"‚Ä¶STT issue: {res.cancellation_details.reason}")
                except Exception:
                    print("‚Ä¶STT canceled.")
        finally:
            # clear pause request so main loop can continue
            stop_for_interrupt.clear()

# ---------- Main debate ----------
async def run_with_saver():
    transcript: list[str] = []

    with wave.open("podcast.wav", "wb") as master_wf:
        master_wf.setnchannels(WAV_CH)
        master_wf.setsampwidth(WAV_SW)
        master_wf.setframerate(WAV_RATE)

        # Greeting (A)
        greeting = "Hi Yashraj, how are you? Ready to listen to a debate on any topic or data? Just paste it below."
        print(f"\nüü¶ {AGENT_A} (greeting): {greeting}\n")
        wav_path = tts_to_temp_wav(greeting, VOICE_A)
        try:
            play_wav_blocking(wav_path)
            append_wav_into_master(wav_path, master_wf)
        finally:
            try: os.remove(wav_path)
            except Exception: pass
        transcript.append(f"{AGENT_A}: {greeting}")

        # Context once (blocking)
        print("üì• Paste your context and press Enter:")
        context = sys.stdin.readline().strip()
        if not context:
            context = "Sample context about your topic."
        print("üìö Context captured. Starting debate...\n")

        # Start hotkey watcher AFTER context is pasted (so it doesn‚Äôt eat that line)
        asyncio.create_task(hotkey_watcher())

        # Warm-up (B)
        warm = await llm(SYSTEM_B, "Agent A greeted the user; reply briefly and warmly, then mention you're ready.")
        print(f"üü© {AGENT_B}: {warm}\n")
        wav_path = tts_to_temp_wav(warm, VOICE_B)
        try:
            play_wav_blocking(wav_path)
            append_wav_into_master(wav_path, master_wf)
        finally:
            try: os.remove(wav_path)
            except Exception: pass
        transcript.append(f"{AGENT_B}: {warm}")

        # Debate loop
        last = f"Context to discuss:\n{context}"

        while True:
            # If user pressed 'i', pause here and capture mic (task handles it)
            if stop_for_interrupt.is_set():
                # wait until mic capture finishes and queue is filled (or cleared)
                while stop_for_interrupt.is_set():
                    await asyncio.sleep(0.02)

            # Drain any pending interrupt text (if user already spoke)
            try:
                while True:
                    intr = interrupt_q.get_nowait()
                    last = f"User interruption: {intr}"
                    transcript.append(f"User: {intr}")
            except asyncio.QueueEmpty:
                pass

            # Agent A
            a_out = await llm(SYSTEM_A, last)
            print(f"\nüü¶ {AGENT_A}: {a_out}\n")
            wav_path = tts_to_temp_wav(a_out, VOICE_A)
            try:
                play_wav_blocking(wav_path)
                append_wav_into_master(wav_path, master_wf)
            finally:
                try: os.remove(wav_path)
                except Exception: pass
            transcript.append(f"{AGENT_A}: {a_out}")

            # Pause point again for immediate new interrupt
            if stop_for_interrupt.is_set():
                while stop_for_interrupt.is_set():
                    await asyncio.sleep(0.02)
            try:
                while True:
                    intr = interrupt_q.get_nowait()
                    last = f"User interruption: {intr}"
                    transcript.append(f"User: {intr}")
            except asyncio.QueueEmpty:
                pass

            # Agent B
            b_in = f"{AGENT_A} said: {a_out}\nOriginal context:\n{context[:1500]}"
            if last.startswith("User interruption:"):
                b_in += f"\n{last}"
            b_out = await llm(SYSTEM_B, b_in)
            print(f"\nüü© {AGENT_B}: {b_out}\n")
            wav_path = tts_to_temp_wav(b_out, VOICE_B)
            try:
                play_wav_blocking(wav_path)
                append_wav_into_master(wav_path, master_wf)
            finally:
                try: os.remove(wav_path)
                except Exception: pass
            transcript.append(f"{AGENT_B}: {b_out}")

            last = b_out
            await asyncio.sleep(0.03)

    # Save transcript after loop ends (Ctrl+C)
    with open("podcast.txt", "w", encoding="utf-8") as f:
        f.write("üéôÔ∏è Podcast Transcript\n\n")
        f.write("\n".join(transcript))
    print("‚úÖ Saved podcast.wav and podcast.txt")

# ---------- Entry ----------
if __name__ == "__main__":
    try:
        asyncio.run(run_with_saver())
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Stopped.")
