# two_agents_livekit.py (updated full version)
# pip install -U livekit azure-cognitiveservices-speech azure-identity openai python-dotenv soundfile

import os, sys, asyncio, json, uuid
from dotenv import load_dotenv; load_dotenv()

# ---- Azure OpenAI (LLM) ----
from openai import AzureOpenAI
AZURE_OPENAI_KEY        = os.getenv("AZURE_OPENAI_KEY") or os.getenv("OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT   = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")
OPENAI_API_VERSION      = os.getenv("OPENAI_API_VERSION", "2024-05-01-preview")
oai = AzureOpenAI(api_key=AZURE_OPENAI_KEY, azure_endpoint=AZURE_OPENAI_ENDPOINT, api_version=OPENAI_API_VERSION)

# ---- Azure Speech (AAD) ----
import azure.cognitiveservices.speech as speechsdk
from azure.identity import ClientSecretCredential
TENANT_ID     = os.getenv("TENANT_ID")
CLIENT_ID     = os.getenv("CLIENT_ID")
CLIENT_SECRET = os.getenv("CLIENT_SECRET")
SPEECH_REGION = os.getenv("SPEECH_REGION", "eastus")
RESOURCE_ID   = os.getenv("RESOURCE_ID")  # optional
COG_SCOPE     = "https://cognitiveservices.azure.com/.default"
if not all([TENANT_ID, CLIENT_ID, CLIENT_SECRET, SPEECH_REGION, AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT]):
    raise RuntimeError("Missing required env vars")

cred = ClientSecretCredential(tenant_id=TENANT_ID, client_id=CLIENT_ID, client_secret=CLIENT_SECRET)
def cog_token_str() -> str:
    raw = cred.get_token(COG_SCOPE).token
    return f"aad#{RESOURCE_ID}#{raw}" if RESOURCE_ID else raw

# ---- LiveKit (client SDK, local tracks) ----
from livekit import rtc
import soundfile as sf   ### NEW for saving podcast.wav

SESSION_ID = os.getenv("SESSION_ID", "demo-session")
AGENT_A    = os.getenv("AGENT_A", "agentA")
AGENT_B    = os.getenv("AGENT_B", "agentB")
VOICE_A    = os.getenv("VOICE_A", "en-US-GuyNeural")
VOICE_B    = os.getenv("VOICE_B", "en-US-AriaNeural")

LIVEKIT_WS    = os.getenv("LIVEKIT_WS")
LIVEKIT_TOKEN = os.getenv("LIVEKIT_TOKEN")
DEMO_LOCAL_ONLY = not (LIVEKIT_WS and LIVEKIT_TOKEN)

SAMPLE_RATE, CHANNELS, FRAME_MS = 24000, 1, 20
SAMPLES_PER_CH  = SAMPLE_RATE * FRAME_MS // 1000
BYTES_PER_FRAME = SAMPLES_PER_CH * CHANNELS * 2

def track_name(agent: str) -> str:
    return f"{SESSION_ID}-{agent}-voice"

async def lk_connect(room: rtc.Room):
    if DEMO_LOCAL_ONLY:
        print("üîå LiveKit: local-only mode (no server).")
        return
    await room.connect(LIVEKIT_WS, LIVEKIT_TOKEN)
    print(f"üîå LiveKit: connected to {LIVEKIT_WS}")

async def publish_pcm_streaming(room: rtc.Room, agent: str, stream: speechsdk.AudioDataStream, buffer_list: list):
    """Stream Azure TTS PCM to a LiveKit track and play locally; also append to buffer_list for final WAV"""
    source = rtc.AudioSource(sample_rate=SAMPLE_RATE, num_channels=CHANNELS)
    track  = rtc.LocalAudioTrack.create_audio_track(track_name(agent), source)
    await room.local_participant.publish_track(track)
    monitor = rtc.AudioFramePlayer()
    track.add_sink(monitor)

    buf = bytearray(BYTES_PER_FRAME * 5)
    while True:
        n = stream.read_data(buf)
        if n == 0:
            break
        chunk = bytes(buf[:n])
        buffer_list.append(chunk)   ### NEW collect for podcast.wav
        for i in range(0, len(chunk), BYTES_PER_FRAME):
            seg = chunk[i:i+BYTES_PER_FRAME]
            if len(seg) < BYTES_PER_FRAME:
                continue
            frame = rtc.AudioFrame(seg, SAMPLE_RATE, CHANNELS, SAMPLES_PER_CH)
            await source.capture_frame(frame)
            await asyncio.sleep(FRAME_MS/1000)

async def tts_stream_to_livekit(room: rtc.Room, agent: str, text: str, voice: str, buffer_list: list):
    cfg = speechsdk.SpeechConfig(auth_token=cog_token_str(), region=SPEECH_REGION)
    cfg.speech_synthesis_voice_name = voice
    cfg.set_speech_synthesis_output_format(speechsdk.SpeechSynthesisOutputFormat.Raw24Khz16BitMonoPcm)
    synth = speechsdk.SpeechSynthesizer(speech_config=cfg, audio_config=None)
    res = await asyncio.to_thread(lambda: synth.speak_text_async(text).get())
    if res.reason != speechsdk.ResultReason.SynthesizingAudioCompleted:
        print(f"‚ùå TTS failed ({agent}): {res.reason}"); return
    stream = speechsdk.AudioDataStream(res)
    await publish_pcm_streaming(room, agent, stream, buffer_list)

# ---- Debate logic ----
SYSTEM_A = "You are Agent A, a friendly analyst. Speak 2‚Äì3 short sentences; reference context; occasionally ask a probing question."
SYSTEM_B = "You are Agent B, a pragmatic strategist. Speak 2‚Äì3 short sentences; build on or challenge A; give next-steps."

async def llm(panel_prompt: str, msg: str) -> str:
    r = await asyncio.to_thread(lambda: oai.chat.completions.create(
        model=AZURE_OPENAI_DEPLOYMENT,
        messages=[{"role":"system","content":panel_prompt},
                  {"role":"user","content":msg}],
        max_tokens=220, temperature=0.8))
    return (r.choices[0].message.content or "").strip()

interrupt_q: asyncio.Queue[str] = asyncio.Queue()

async def read_interrupts():
    print("‚è∏  Type an interruption line anytime and press Enter to cut in.")
    while True:
        try:
            line = await asyncio.to_thread(sys.stdin.readline)
            if not line:
                await asyncio.sleep(0.1); continue
            text = line.strip()
            if text:
                await interrupt_q.put(text)
        except Exception:
            await asyncio.sleep(0.2)

async def debate(room: rtc.Room, context: str, script_path="podcast_script.txt"):
    """Alternate A/B turns; speak; allow console interrupts; log transcript; run until Ctrl+C"""
    buffers = []   ### NEW store PCM chunks for WAV
    with open(script_path, "w", encoding="utf-8") as f:
        f.write("üéôÔ∏è Podcast Transcript\n\n")

    last = f"Context to discuss:\n{context}"
    turn = 0
    while True:   ### NEW no limit, manual stop with Ctrl+C
        turn += 1
        intr = None
        try: intr = interrupt_q.get_nowait()
        except asyncio.QueueEmpty: pass
        if intr: last = f"User interruption: {intr}"

        a_in  = last
        a_out = await llm(SYSTEM_A, a_in)
        print(f"\nüü¶ A: {a_out}\n")
        await tts_stream_to_livekit(room, AGENT_A, a_out, VOICE_A, buffers)
        with open(script_path, "a", encoding="utf-8") as f: f.write(f"A: {a_out}\n")

        intr = None
        try: intr = interrupt_q.get_nowait()
        except asyncio.QueueEmpty: pass
        b_in = f"Agent A said: {a_out}\nOriginal context:\n{context[:1500]}"
        if intr: b_in += f"\nUser interruption: {intr}"
        b_out = await llm(SYSTEM_B, b_in)
        print(f"\nüü© B: {b_out}\n")
        await tts_stream_to_livekit(room, AGENT_B, b_out, VOICE_B, buffers)
        with open(script_path, "a", encoding="utf-8") as f: f.write(f"B: {b_out}\n")

        last = b_out

    # never reaches here unless stopped, then save podcast.wav
    pcm = b''.join(buffers)
    sf.write("podcast.wav", pcm, SAMPLE_RATE, "PCM_16")

async def main():
    ctx_file = os.getenv("CONTEXT_FILE", "").strip()
    if ctx_file and os.path.exists(ctx_file):
        with open(ctx_file, "r", encoding="utf-8", errors="ignore") as f: context = f.read()
    else:
        context = os.getenv("CONTEXT_TEXT", "Sample context about your topic.")

    room = rtc.Room()
    await lk_connect(room)

    if not DEMO_LOCAL_ONLY:
        @room.on("dataReceived")
        def _on_data(pkt: rtc.DataPacket):
            try: msg = json.loads(pkt.data.decode())
            except Exception: msg = {"raw": pkt.data.decode(errors="ignore")}
            print("üì• DC in:", msg)

    asyncio.create_task(read_interrupts())
    print("\n‚ñ∂Ô∏è  Starting two-agent podcast (Ctrl+C to stop)...")
    try:
        await debate(room, context)
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Stopped by user, saving podcast.wav")
        # debate() saves transcript; audio already in buffers

if __name__ == "__main__":
    asyncio.run(main())
